% ============================================================================
% Chapter 5: Results
% ============================================================================

\chapter{Results}
\label{ch:results}

This chapter provides an exhaustive experimental evaluation of the AI2MCP architecture, quantifying its performance across relational reasoning, system latency, and safety verification. The findings validate the core thesis that a protocol-driven approach (MCP) enables swappable robotic intelligence without compromising real-time performance.

\section{Experimental Configuration}

\subsection{Dataset and Task Context}
The architecture was evaluated using the \textit{lerobot/aloha\_static\_coffee} dataset, comprising 55,000 frames of bimanual kinematic data. This benchmark represents a kinematically-rich environment where spatial predicates must be inferred from 14-DoF joint state vectors. 

\subsection{Infrastructure and Hardware Profiles}
Evaluations were benchmarked on two primary profiles: a high-performance development environment (NVIDIA RTX 3070, 8GB VRAM) and an edge-constrained deployment target (NVIDIA RTX 500 Ada, 4GB VRAM). Training cycles for the relational models were completed in approximately 29 minutes for 100 epochs on the full 55k-frame dataset.

\section{Relational Reasoning Performance}

\subsection{Global Predicate Accuracy}
The RelationalGNN reasoning engine achieved a validation accuracy of \textbf{97.03\%} during training. As illustrated in \Cref{fig:training-curves}, the model exhibits rapid convergence with minimal loss variance, reaching optimal performance by epoch 69.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/training_curves.png}
    \caption{RelationalGNN training on 55,000 ALOHA frames showing high stability in both loss convergence and micro-accuracy.}
    \label{fig:training-curves}
\end{figure}

\subsection{Inference Benchmark Results}
The trained RelationalGNN was evaluated on a 500-frame holdout set, demonstrating substantial improvements over locally-trained baselines. Table~\ref{tab:inference-benchmark} summarizes the inference performance.

\begin{table}[htbp]
\centering
\caption{Inference Benchmark Results (500 frames, remote-trained model)}
\label{tab:inference-benchmark}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Local (5k frames)} & \textbf{Remote (55k frames)} \\
\midrule
Pass@1 & 88.17\% & \textbf{98.99\%} \\
Pass@3 & 98.16\% & \textbf{99.95\%} \\
Predicate Accuracy & 92.48\% & \textbf{98.45\%} \\
Predicate F1 & 82.12\% & \textbf{95.15\%} \\
Precision & --- & 91.13\% \\
Recall & --- & 99.54\% \\
\bottomrule
\end{tabular}
\end{table}

The \textbf{+10.8\%} improvement in Pass@1 and \textbf{+13.0\%} improvement in F1 score demonstrate the importance of training on the full dataset rather than subsets. These results validate the architecture's capacity for high-fidelity relational reasoning when provided with sufficient training data.

\subsection{Radar-Based Multi-Axis Comparison}
To emphasize the architectural advantages of a protocol-driven GNN over heavy multimodal stacks, we utilize a radar chart to compare Option A (RelationalGNN) and Option C (MultiModalGNN).

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/radar_comparison.png}
    \caption{Radar chart evaluating the trade-offs between kinematic reasoning (A) and multimodal fusion (C). Option A dominates on latency, memory, and model size.}
    \label{fig:radar-comparison}
\end{figure}

\section{Detailed Comparative Metrics}

\subsection{Fair Architecture Comparison (Option A vs. C)}
Following a fair comparison on the full 55,000-frame dataset, it was observed that vision-augmented features (DINOv2) did not improve spatial predicate accuracy in the structured ALOHA environment. Table~\ref{tab:fair-comparison} summarizes these results.

\begin{table}[htbp]
\centering
\caption{Final Benchmark Comparison (55k frames)}
\label{tab:fair-comparison}
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{RelationalGNN (A)} & \textbf{MultiModalGNN (C)} & \textbf{Winner} \\
\midrule
Micro Accuracy & \textbf{97.03\%} & 96.51\% & A \\
Macro F1 Score & \textbf{0.358} & 0.348 & A \\
Inference Latency & \textbf{1.52ms} & 24.29ms & A ($16\times$) \\
Peak Memory (MB) & \textbf{19.4} & 141.8 & A ($7\times$) \\
Model Size (MB) & \textbf{0.81} & 2.14 & A \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Per-Predicate Distribution and the Interaction Gap}
Detailed analysis of the predicate distribution reveals high fidelity in spatial predicates like \texttt{is\_near} (F1: 0.954) and \texttt{is\_left\_of} (F1: 0.969). However, an ``Interaction Predicate Gap'' exists for contact-based predicates (\texttt{is\_holding}), which reported 0.000 F1 on real data due to a lack of annotated contact signals in the kinematic stream.

\section{System Performance and Scalability}

\subsection{Honest Latency and Perception Bottlenecks}
A critical finding of this research is the latency disparity between standardized protocol reasoning and raw perception. As shown in \Cref{fig:latency-distribution}, total E2E latency (297--332ms) is dominated by detection (234ms) rather than the GNN reasoner or MCP serialization.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/latency_distribution.png}
    \caption{Distribution of inference latencies across 500 evaluation frames. MCP overhead remains minimal (~0.6ms), ensuring high throughput.}
    \label{fig:latency-distribution}
\end{figure}

\subsection{Protocol Throughput}
The MCP bridge maintained a throughput of \textbf{16.9 operations per second}, facilitating real-time ``agent-aware'' robotic control. 

\section{Safety and Temporal Verifiers}

\subsection{Pre-Execution Simulation (Forward Dynamics)}
The Forward Dynamics Model, trained as a protocol-level safety guardrail on the full 55,000-frame dataset, achieved a state-prediction error of \textbf{$\delta = 0.0017$}. By utilizing RAM pre-computation, a \textbf{730$\times$ speedup} in training time was achieved.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/forward_dynamics_speedup.png}
    \caption{Training speedup analysis showing a 730$\times$ reduction in epoch time via RAM pre-computation.}
    \label{fig:dynamics-speedup}
\end{figure}

The \texttt{simulate\_action} tool provides recommendations based on an internal confidence range of \textbf{0.49â€“0.62}. The model is conservative by design; while the default threshold is 0.7, a lower threshold of \textbf{0.5} is recommended for production environments to balance safety and task fluidity.

\subsection{Temporal Stability with ST-GNN}
To mitigate predicate ``flickering'' across consecutive frames, a Spatiotemporal GNN (ST-GNN) was trained. While achieving ~90\% accuracy, the ST-GNN provides a smoother relational belief state for the agentic planner. A summary of the predictive stability and dynamics is visualized in \Cref{fig:phase-summary}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/phase_10_11_summary.png}
    \caption{Summary of predictive stability and dynamics performance across Phase 10 and 11.}
    \label{fig:phase-summary}
\end{figure}

\section{Agentic Validation}

\subsection{Swappable Brain Performance (Pass@k)}
The ability to swap between Large Language Models was validated using a 10-goal benchmark suite. Both local \textbf{Llama 3.2} and \textbf{Qwen 2.5} agents achieved a \textbf{100\% success rate} through the standardized MCP interface, proving that robotic intelligence can be treated as a decoupled network service.

\begin{table}[htbp]
\centering
\caption{Swappable Agent Efficiency Benchmark}
\label{tab:agent-efficiency}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Llama 3.2 (3B)} & \textbf{Qwen 2.5 (3B)} \\
\midrule
Success Rate & 100\% & 100\% \\
Avg. Steps to Completion & 2.8 & \textbf{1.0} \\
Avg. Total Time (ms) & 2561 & \textbf{1537} \\
Time to First Token (ms) & \textbf{425} & 1073 \\
\bottomrule
\end{tabular}
\end{table}

As visualized in \Cref{fig:pass-at_k}, the GNN reasoner achieves \textbf{99.0\% Pass@1} and approaches 100\% at higher $k$ values. While Llama 3.2 demonstrated faster initial response times (time-to-first-token), Qwen 2.5 was \textbf{40\% faster overall} and utilized \textbf{65\% fewer steps} to reach task completion.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/pass_at_k.png}
    \caption{Pass@k analysis showing 99.0\% single-attempt success rate, validating swappable intelligence via the standardized MCP bridge.}
    \label{fig:pass-at_k}
\end{figure}

\section{Key Results Summary}
\begin{itemize}
    \item \textbf{Relational Fidelity}: 99.0\% Pass@1, 95.2\% F1 in spatial predicate reasoning from kinematics (97.03\% training accuracy).
    \item \textbf{Decoupling Success}: Swappable LLM agents (Llama and Qwen) achieved 100\% success rate in 1.5--2.6s cycles.
    \item \textbf{Computational Advantage}: RelationalGNN is $16\times$ faster and $7\times$ leaner than vision-fusion alternatives.
    \item \textbf{Safety Assurance}: Protocol-level verification via forward dynamics rollouts achieved a prediction error of $\delta = 0.0017$.
    \item \textbf{Training Scale Impact}: Full 55k-frame training improved Pass@1 by +10.8\% and F1 by +13.0\% over local 5k-frame baselines.
\end{itemize}