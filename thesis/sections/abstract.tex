% ============================================================================
% Abstract
% ============================================================================

\begin{abstract}

    The recent proliferation of Large Language Models (LLMs) has catalyzed a paradigm shift toward embodied AI, yet the field remains obstructed by the ($N \times M$) integration problem, where the development of $N$ models for $M$ robot platforms requires bespoke, platform-specific implementations \cite{lee_robot_2025, chhetri_model_2025}. This thesis introduces a novel, runtime-agnostic middleware architecture for robotic control based on the Model Context Protocol (MCP), designed to decouple high-level intelligence from hardware-specific implementations. We implement a novel MCP-to-ROS~2 bridge that exposes robotic affordances as a set of 13 standardized tools and 13 resources, enabling any MCP-compatible agent to orchestrate complex behaviors through a unified interface.
    
    Central to this architecture is a Relational Graph Neural Network (RelationalGNN) that transforms raw 14-Degree-of-Freedom (DoF) joint state vectors into structured semantic predicates. Our experimental evaluation on the 55,000-frame ALOHA dataset demonstrates that the RelationalGNN achieves a validation accuracy of \textbf{97.03\%}, outperforming multimodal fusion alternatives in structured environments while being $16\times$ faster (1.5ms latency) and $7\times$ more memory-efficient. The protocol bridge maintains a throughput of \textbf{16.9 operations per second}, facilitating real-time interactive control \cite{fu_rosbag_2025}.
    
    To validate the thesis claim of swappable intelligence, we conducted an agentic benchmark suite using multiple local models. We demonstrate that local models such as Llama 3.2 and Qwen 2.5 can be hot-swapped with zero modifications to the robotic control stack, both achieving \textbf{100\% success rates} in protocol-driven task navigation. Furthermore, we introduce a learned Forward Dynamics model that serves as a protocol-level ``Physicality Filter,'' achieving a state-prediction error of $\delta = 0.0017$ to reject kinematically impossible plans before hardware execution \cite{roth_learned_2025}. 
    
    Despite these successes, we identify a critical ``Interaction Predicate Gap,'' where kinematics-only reasoning fails to resolve contact-based predicates (\texttt{is\_holding}, F1: 0.000) due to dataset-level label imbalance and the absence of haptic feedback. This work establishes the foundational infrastructure for protocol-driven robotics, offering a scalable path toward interoperable, auditable, and hardware-agnostic embodied AI \cite{lee_robot_2025, grey_understanding_2025}.
    
    \vspace{1em}
    \noindent\textbf{Keywords:} Model Context Protocol (MCP), Robot Operating System 2 (ROS 2), Relational Graph Neural Networks, Embodied AI, Swappable Intelligence, Forward Dynamics.
    
    \end{abstract}