% ============================================================================
% Abstract
% ============================================================================

\begin{abstract}

This thesis introduces a novel, runtime-agnostic middleware architecture for robotic control based on the Model Context Protocol, designed to decouple high-level intelligence from hardware-specific implementations and dispense with hardcoded integration patterns. The proposed system facilitates a structured data flow where joint kinematics are mapped into a relational graph, enabling a multi-layer message-passing graph neural network to infer spatial predicates from raw sensor streams. These predicates are subsequently utilized by large language model agents to support evidence-based task planning in physical environments.

The core contributions of this work are three-fold: the development of a standardized protocol for middleware decoupling, a relational predicate extractor for automated scene reasoning, and a learned forward dynamics model for pre-execution feasibility verification. The architecture includes conventional tool-specific logging that promotes system transparency, offering an accessible layer for evaluating autonomous agents' actions. By integrating semantic graph reasoning with predictive dynamics, this framework establishes a robust foundation for safe and efficient agent-aware robot control during autonomous task execution.

\vspace{1em}
\noindent\textbf{Keywords:} Model Context Protocol, Robot Operating System, Graph Neural Networks, Middleware Architecture, Imitation Learning, Human-Robot Interaction

\end{abstract}
