================================================================================
AI2MCP PROJECT CONTEXT DUMP
Generated: 2025-01-05
Last Updated: 2026-01-18
Purpose: AI context transfer document
================================================================================

[PROJECT_METADATA]
title: A Standardized Middleware Architecture for Decoupled Robotic Intelligence using the Model Context Protocol (MCP)
repo_path: /home/khaled.sazzad/Downloads/Thesis/AI2MCP
language: Python 3.10
framework: PyTorch 2.0+, PyTorch Geometric 2.4+
transport: SSE (Server-Sent Events) + JSON-RPC 2.0
robot_framework: ROS 2 Humble (optional, mock mode available)
dataset: LeRobot ALOHA (lerobot/aloha_static_coffee)

[HARDWARE_SPECIFICATIONS]
training_gpu: NVIDIA RTX 500 Ada Generation Laptop GPU
training_gpu_vram: 4GB (3.9GB usable)
training_gpu_architecture: Ada Lovelace
alternative_gpu: NVIDIA RTX 4080 (16GB VRAM) - user has access
cpu: Not specified, Linux x86_64
os: Linux 6.8.0-90-generic
shell: /usr/bin/zsh

[VRAM_BUDGET_ANALYSIS]
# Critical: Vision stack VRAM requirements vs RTX 500 (4GB) limit
heavy_stack:
  grounding_dino: ~1.5GB
  zoedepth: ~1.2GB
  dinov2_vits14: ~1.0GB
  multimodal_gnn: ~0.1GB
  total: ~3.8GB
  status: ⚠️ At limit, may OOM with batch>1

edge_native_stack:
  yolo_world_s: ~0.5GB
  depth_anything_v2_small: ~0.3GB
  relational_gnn: ~0.1GB
  total: ~0.9GB
  status: ✅ Fits with 3GB headroom

recommendation:
  development: Use MockVisionDetector/MockDepthEstimator
  training: Use RTX 4080 (16GB) for full model training
  deployment_rtx500: Use edge-native stack (YOLO-World + DepthAnything)

[SOFTWARE_ENVIRONMENT]
python_version: 3.10
virtual_env: .venv (project local)
package_manager: pip with pyproject.toml
key_dependencies:
  - torch>=2.0.0
  - torch-geometric>=2.4.0
  - mcp>=1.0.0
  - lerobot>=0.4.2
  - starlette
  - uvicorn
  - httpx
  - structlog
  - numpy
  - Pillow

[PROJECT_STRUCTURE]
src/
  mcp_ros2_bridge/
    __init__.py
    server.py              # Main MCP server, SSE transport, Starlette app
    ros_node.py            # ROS2Bridge class, mock mode fallback
    tools/
      __init__.py          # register_tools(), consolidated handler
      motion.py            # get_motion_tools(), handle_motion_tool()
      perception.py        # get_perception_tools(), handle_perception_tool()
      prediction.py        # PredictionToolsManager, get_prediction_tools(), handle_prediction_tool()
    resources/
      __init__.py          # register_resources(), consolidated handler
      pose.py              # get_pose_resources(), handle_pose_resource()
      scan.py              # get_scan_resources(), handle_scan_resource()
      world_graph.py       # get_world_graph_resources(), handle_world_graph_resource()
      lerobot_state.py     # LeRobotResourceManager class
  agents/
    __init__.py
    base_agent.py          # MCPClient, AgentState, BaseAgent
    llama_agent.py         # LlamaAgent (Ollama)
    qwen_agent.py          # QwenAgent (Ollama) - NEW
  gnn_reasoner/
    __init__.py            # exports DataManager, LeRobotGraphTransformer, ALOHA_KINEMATIC_CHAIN
    data_manager.py        # DataManager class wrapping LeRobotDataset
    lerobot_transformer.py # LeRobotGraphTransformer, compute_heuristic_predicates, add_predicate_labels
    benchmark.py           # BenchmarkLogger class
    model/
      __init__.py
      scene_gnn.py         # Original SceneGNN (deprecated)
      graph_encoder.py     # GraphEncoder
      relational_gnn.py    # RelationalGNN, NodeEncoder, EdgeEncoder, PredicateHead
      multimodal_gnn.py    # MultiModalGNN, VisionEncoder, CrossAttentionFusion
      forward_dynamics.py  # ForwardDynamicsModel (Phase 10.3)
      spatiotemporal_gnn.py # SpatiotemporalGNN, TemporalGRU (Phase 11)
scripts/
  train_relational_gnn.py      # Training script with GPU profiles
  train_multimodal_gnn.py      # MultiModalGNN training script
  train_forward_model.py       # ForwardDynamicsModel training (Phase 10.3)
  train_spatiotemporal_gnn.py  # SpatiotemporalGNN training (Phase 11)
  demo_lerobot_pipeline.py     # Full pipeline demo
  demo_mcp_client.py           # MCP client E2E demo
  generate_thesis_figures.py   # Matplotlib figure generation
  compare_models.py            # Option A vs C benchmark
  test_mcp_connection.py       # Basic connection test
experiments/
  aloha_training/
    best_model.pt              # Best checkpoint (epoch 15)
    final_model.pt             # Final checkpoint (epoch 100)
    training_history.json      # Loss/accuracy curves
figures/
  training_curves.pdf/.png
  architecture.pdf/.png
  learning_rate.png
  predicate_distribution.png
  comparison_table.png
thesis/
  main.tex
  references.bib
  sections/*.tex

[MCP_TOOLS_REGISTERED]
total_tools: 14
motion_tools:
  - move: linear_x, angular_z, duration_ms -> moves robot
  - set_velocity: linear_x, angular_z -> continuous velocity
  - stop: emergency halt
  - rotate: angle_degrees, speed -> rotation
  - move_forward: distance_meters, speed -> forward motion
perception_tools:
  - get_obstacle_distances: directions[] -> distances by direction
  - check_path_clear: distance_meters, width_meters -> is_clear bool
  - scan_surroundings: num_sectors -> 360deg obstacle summary
prediction_tools:
  - get_world_graph: frame_idx, threshold -> GNN world context
  - predict_action_outcome: action[], num_steps -> future predicates
  - advance_frame: -> next dataset frame
  - set_frame: frame_idx -> set specific frame
  - get_predicates: threshold -> active predicates
  - simulate_action: action_sequence[], confidence_threshold -> EXECUTE/REPLAN recommendation (Phase 10.3)

[MCP_RESOURCES_REGISTERED]
total_resources: 13
pose_resources:
  - robot://pose: x, y, theta position
  - robot://velocity: linear_x, angular_z
  - robot://status: connected, is_moving, has_scan
scan_resources:
  - robot://scan/summary: quadrant distances
  - robot://scan/obstacles: clustered obstacle list
  - robot://scan/raw: raw lidar ranges
world_graph_resources:
  - robot://world_graph: nodes, edges, metadata
  - robot://world_graph/entities: node list
  - robot://world_graph/relations: edge list
lerobot_resources:
  - robot://lerobot/current_state: observation state
  - robot://lerobot/world_graph: GNN-processed graph
  - robot://lerobot/predicates: active predicates
  - robot://lerobot/dataset_info: repo_id, frame count

[GNN_ARCHITECTURE]
model_name: RelationalGNN
base_architecture: GATv2 (Graph Attention Network v2)
components:
  - NodeEncoder: MLP(5 -> hidden_dim), input=[x,y,z,theta,type]
  - EdgeEncoder: MLP(2 -> hidden_dim), input=[distance, angle]
  - GNNLayers: 3x GATv2Conv with 4 heads, residual connections
  - PredicateHead: MLP(hidden_dim*2 -> num_predicates) pairwise
  - GraphPool: global_mean_pool for graph-level embedding
hyperparameters:
  hidden_dim: 128
  num_layers: 3
  num_heads: 4
  dropout: 0.1
  num_predicates: 9
total_parameters: 203,369

[PREDICATE_DEFINITIONS]
spatial_predicates:
  - is_near: distance < 0.2m
  - is_above: delta_z > 0.1m
  - is_below: delta_z < -0.1m
  - is_left_of: delta_x < -0.05m
  - is_right_of: delta_x > 0.05m
interaction_predicates:
  - is_holding: gripper(type=1) near object(type=2), dist < 0.1m
  - is_contacting: distance < 0.05m
  - is_approaching: velocity toward target (negative distance change)
  - is_retracting: velocity away from target (positive distance change)

[ALOHA_KINEMATIC_CHAIN]
robot: ALOHA bimanual manipulator
total_dof: 14 (7 per arm)
joints_per_arm:
  0: waist (revolute)
  1: shoulder (revolute)
  2: elbow (revolute)
  3: forearm_roll (revolute)
  4: wrist_angle (revolute)
  5: wrist_rotate (revolute)
  6: gripper (prismatic)
state_vector: 14 floats (joint angles)
action_vector: 14 floats (joint velocities)

[TRAINING_RESULTS_SYNTHETIC]
dataset_size: 1000 samples
epochs: 50
total_time: 21.1s
time_per_epoch: 0.42s
best_val_loss: 0.1086
final_val_accuracy: 0.9593 (95.9%)
gpu_profile: RTX 500 Ada
batch_size: 16
accumulation_steps: 4
use_amp: true

[TRAINING_RESULTS_REAL_ALOHA]
dataset: lerobot/aloha_static_coffee
dataset_total_frames: 55,000
frames_used: 5,000
epochs: 100
total_time: 204.9s (3.4 minutes)
time_per_epoch: 2.05s
best_val_loss: 0.0232
best_epoch: 15
final_val_accuracy: 0.9942 (99.4%)
train_samples: 4,500
val_samples: 500
gpu_profile: RTX 500 Ada
batch_size: 16
effective_batch_size: 64
accumulation_steps: 4
use_amp: true
optimizer: AdamW
learning_rate_initial: 3e-4
learning_rate_final: 1e-6
scheduler: CosineAnnealingLR
weight_decay: 1e-5

[INFERENCE_BENCHMARKS]
# Updated: 2026-01-18 with remote-trained model
data_source: lerobot/aloha_static_coffee
model: experiments/remote_training/relational_gnn/best_model.pt
frames_processed: 500
device: cpu (local benchmark with RTX 3070-trained model)

timing_metrics_ms:
  inference_latency_mean: 17.83
  inference_latency_p95: 32.19
  graph_construction_mean: 4.11
  graph_construction_p95: 8.73
  serialization_mean: 0.26
  serialization_p95: 0.38
  total_request_mean: 42.88
  total_request_p95: 75.72
protocol_overhead: 58.4%

accuracy_metrics:
  pass@1: 0.9899 (98.99%)
  pass@3: 0.9995 (99.95%)
  pass@5: 0.9995 (99.95%)
  pass@10: 0.9995 (99.95%)
  
predicate_classification:
  accuracy: 0.9846 (98.46%)
  precision: 0.9113 (91.13%)
  recall: 0.9954 (99.54%)
  f1: 0.9515 (95.15%)

benchmark_file: experiments/benchmark_remote_model.json

benchmark_improvement_summary:
  # Comparison: Local (5k frames) vs Remote-Trained (55k frames)
  pass@1: 88.17% → 98.99% (+10.8%)
  pass@3: 98.16% → 99.95% (+1.8%)
  predicate_accuracy: 92.48% → 98.45% (+6.0%)
  predicate_f1: 82.12% → 95.15% (+13.0%)
  figures_updated: true

[MCP_E2E_DEMO_RESULTS]
tools_discovered: 13
resources_discovered: 13
session_init: success
avg_roundtrip_ms: 59.1
p95_roundtrip_ms: 68.1
throughput_ops_sec: 16.9
benchmark_cycles: 10 (advance_frame + get_world_graph)

[PREDICATE_DETECTION_RESULTS]
test_frames: 100
total_spatial_predicates: 9,835
avg_spatial_per_frame: 98.3
predicate_breakdown:
  is_near: 4,658 detections
  is_left_of: 2,586 detections
  is_right_of: 2,591 detections
  is_above: 0 detections (horizontal workspace)
  is_below: 0 detections (horizontal workspace)
total_interaction_predicates: 0
interaction_detection_issue: label_imbalance, missing_object_nodes

[PROBLEMS_ENCOUNTERED_AND_SOLUTIONS]

problem_1:
  description: ModuleNotFoundError: No module named 'lerobot.common'
  cause: LeRobot package restructured in v0.4.x
  old_import: from lerobot.common.datasets.lerobot_dataset import LeRobotDataset
  new_import: from lerobot.datasets.lerobot_dataset import LeRobotDataset
  file_fixed: src/gnn_reasoner/data_manager.py
  solution: Updated import paths to match lerobot 0.4.2 structure

problem_2:
  description: RuntimeError: Parent directory experiments/training does not exist
  cause: Training script tried to save checkpoints before creating directory
  file_fixed: scripts/train_relational_gnn.py
  solution: Added output_dir.mkdir(parents=True, exist_ok=True) at start of train()

problem_3:
  description: AttributeError: 'Server' object has no attribute '_tool_handlers'
  cause: MCP library internal API changed, code tried to access private handlers
  files_fixed: 
    - src/mcp_ros2_bridge/tools/__init__.py
    - src/mcp_ros2_bridge/tools/motion.py
    - src/mcp_ros2_bridge/tools/perception.py
    - src/mcp_ros2_bridge/tools/prediction.py
    - src/mcp_ros2_bridge/resources/__init__.py
    - src/mcp_ros2_bridge/resources/pose.py
    - src/mcp_ros2_bridge/resources/scan.py
    - src/mcp_ros2_bridge/resources/world_graph.py
  solution: Refactored to consolidated registration pattern - single list_tools/call_tool handler that routes to module-specific handlers. Each module now exports get_X_tools() and handle_X_tool() functions.

problem_4:
  description: RuntimeError: Expected all tensors to be on the same device, cuda:0 and cpu
  cause: GNN model on GPU, graph data on CPU
  files_fixed:
    - src/mcp_ros2_bridge/tools/prediction.py
    - src/mcp_ros2_bridge/resources/lerobot_state.py
    - scripts/demo_lerobot_pipeline.py
  solution: Added device = next(model.parameters()).device; graph = graph.to(device) before inference

problem_5:
  description: Interaction predicates always 0 even on real data
  cause: Label imbalance (~95% negative), model learned to predict 0 always
  root_cause: Kinematic-only graph lacks object nodes (type=2), is_holding requires gripper-object edges
  status: Documented as limitation, requires visual object detection for fix
  recommendation: Extend graph with object nodes from vision pipeline

problem_6:
  description: Large dataset download times (ALOHA videos ~2GB)
  cause: LeRobot streams video files on first access
  solution: Added --synthetic flag for quick validation, recommend background download

problem_7:
  description: prev_graph device mismatch in demo pipeline
  cause: Graph moved to GPU, then used as prev_graph for next iteration on CPU
  file_fixed: scripts/demo_lerobot_pipeline.py
  solution: Store CPU copy before moving to GPU: graph_cpu = graph.clone(); graph = graph.to(device); prev_graph = graph_cpu

[GPU_PROFILE_CONFIGURATIONS]
rtx500_profile:
  name: RTX 500 Ada (4GB)
  batch_size: 16 (relational) / 8 (multimodal)
  effective_batch_size: 64
  accumulation_steps: 4 / 8
  use_amp: true
  vram_threshold: < 6GB
  
rtx3070_profile:
  name: RTX 3070 (8GB)
  batch_size: 32 (relational) / 16 (multimodal)
  effective_batch_size: 64
  accumulation_steps: 2 / 4
  use_amp: true
  vram_threshold: 6-12GB
  
rtx4080_profile:
  name: RTX 4080 (16GB)
  batch_size: 64 (relational) / 32 (multimodal)
  effective_batch_size: 64
  accumulation_steps: 1 / 2
  use_amp: true
  vram_threshold: >= 12GB

cpu_profile:
  name: CPU Fallback
  batch_size: 8 (relational) / 4 (multimodal)
  effective_batch_size: 64
  accumulation_steps: 8 / 16
  use_amp: false

[KEY_CODE_PATTERNS]

pattern_consolidated_tool_registration:
```python
def register_tools(server, ros_bridge, prediction_tools_manager=None):
    all_tools = get_motion_tools() + get_perception_tools()
    if prediction_tools_manager:
        all_tools += get_prediction_tools()
    
    @server.list_tools()
    async def list_all_tools():
        return all_tools
    
    @server.call_tool()
    async def call_tool(name, arguments):
        if name in motion_names:
            return await handle_motion_tool(name, arguments, ros_bridge)
        # ... route to other handlers
```

pattern_device_handling:
```python
device = next(self.gnn_model.parameters()).device
graph_data = self.graph_transformer.to_graph(state)
graph_data = graph_data.to(device)
with torch.no_grad():
    context = self.gnn_model.to_world_context(graph_data, threshold)
```

pattern_gradient_accumulation:
```python
for i, batch in enumerate(dataloader):
    with autocast(enabled=use_amp):
        loss = criterion(model(batch), batch.y) / accumulation_steps
    scaler.scale(loss).backward()
    if (i + 1) % accumulation_steps == 0:
        scaler.step(optimizer)
        scaler.update()
        optimizer.zero_grad()
```

pattern_auto_model_loading:
```python
auto_paths = [
    Path("experiments/aloha_training/best_model.pt"),
    Path("experiments/training/best_model.pt"),
]
for path in auto_paths:
    if path.exists():
        checkpoint = torch.load(path, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint["model_state_dict"])
        break
```

[ENVIRONMENT_VARIABLES]
LEROBOT_ENABLED: "true"/"false" - enable LeRobot integration
LEROBOT_REPO_ID: dataset repo (default: lerobot/aloha_static_coffee)
LEROBOT_MODEL_PATH: path to trained model checkpoint

[CLI_COMMANDS]

start_server:
  python -m mcp_ros2_bridge.server --lerobot
  python -m mcp_ros2_bridge.server --lerobot --lerobot-model experiments/aloha_training/best_model.pt

train_synthetic:
  python scripts/train_relational_gnn.py --synthetic --epochs 50

train_real:
  python scripts/train_relational_gnn.py --repo lerobot/aloha_static_coffee --epochs 100 --max-frames 5000 --output experiments/aloha_training

train_multimodal:
  python scripts/train_multimodal_gnn.py --repo lerobot/aloha_static_coffee --epochs 100 --output experiments/multimodal

train_forward_dynamics:
  # Quick test (RTX 500)
  python scripts/train_forward_model.py --max-frames 1000 --epochs 10
  # Full training (RTX 3070 recommended)
  python scripts/train_forward_model.py --repo lerobot/aloha_static_coffee --epochs 100 --gnn-checkpoint experiments/aloha_training/best_model.pt --output experiments/forward_dynamics

demo_pipeline:
  python scripts/demo_lerobot_pipeline.py --synthetic --frames 100
  python scripts/demo_lerobot_pipeline.py --repo lerobot/aloha_static_coffee --frames 100 -v

demo_mcp:
  python scripts/demo_mcp_client.py

compare_models:
  python scripts/compare_models.py --model-a experiments/aloha_training/best_model.pt --model-c experiments/multimodal_aloha/best_model.pt --frames 500

generate_figures:
  python scripts/generate_thesis_figures.py
  python scripts/generate_comparison_figures.py

[THESIS_STRUCTURE]
chapters:
  1. Introduction: motivation, research question, contributions
  2. Background: MCP, ROS2, GNN, LeRobot, related work
  3. Methodology: architecture, tools, resources, GNN design
  4. Implementation: code structure, training pipeline
  5. Results: training curves, predicates, latency, throughput
  6. Discussion: implications, limitations, future work
  7. Conclusion: summary, contributions

key_thesis_claims:
  - MCP solves N×M integration problem
  - GNN provides structured relational understanding
  - 99.0% pass@1, 95.2% F1 demonstrates viability (remote-trained model)
  - 17.8ms mean inference latency (real-time viable)
  - Protocol overhead ~58% (includes dataset I/O)
  - ⚠️ REVISED: Vision integration does NOT improve accuracy on ALOHA dataset
    - Previous claim: +35.6% is_near (0.668 → 0.906) was before WeightedFocalLoss fix
    - Current result: RelationalGNN BEATS MultiModalGNN (0.954 vs 0.920)
  - Latency trade-off: RelationalGNN 16× faster (1.5ms vs 24ms)
  - Swappable AI brain VALIDATED: Multiple LLMs tested via MCP → GNN E2E
    - Llama3.2 (3B): 3 steps, 5.3s
    - Qwen2.5 (3B): JSON parsing validated

[FUTURE_WORK_ITEMS]
1. ✅ Visual object integration (YOLO/DETIC → object nodes) - COMPLETE
2. Edge-native perception stack (YOLO-World + Depth Anything V2 Small) - for RTX 500 deployment
3. LLM agent loop with chain-of-thought
4. Multi-robot orchestration via MCP
5. Safety constraint verification
6. Temporal GNN for motion predicates
7. Physical robot deployment
8. Benchmark suite standardization

[REPOSITORY_URLS]
lerobot_dataset: huggingface.co/datasets/lerobot/aloha_static_coffee
mcp_spec: modelcontextprotocol.io
pytorch_geometric: pyg.org

[CHECKPOINT_CONTENTS]
best_model.pt:
  - model_state_dict: RelationalGNN weights
  - optimizer_state_dict: AdamW state
  - epoch: 15
  - val_loss: 0.0232
  - val_accuracy: 0.9942

training_history.json:
  - profile: "RTX 500 Ada (4GB)"
  - epochs: 100
  - total_time_seconds: 204.9
  - best_val_loss: 0.0232
  - final_val_accuracy: 0.9942
  - history.train_loss: [100 floats]
  - history.val_loss: [100 floats]
  - history.val_accuracy: [100 floats]
  - history.learning_rate: [100 floats]

[SESSION_COMPLETED_TASKS]
1. ✅ GNN model training (99.4% accuracy)
2. ✅ Model integration into MCP server
3. ✅ MCP server refactoring (fixed tool/resource registration)
4. ✅ E2E demo client creation and testing
5. ✅ Thesis figure generation (5 figures)
6. ✅ Complete LaTeX thesis package (7 chapters)
7. ✅ BibTeX references (25+ entries)
8. ✅ Vision integration (MultiModalGNN, +35.6% is_near F1)
9. ✅ LLM agent integration (Llama3.2 via Ollama)
10. ✅ Thesis core demo validated (LLM → MCP → GNN E2E)
11. ✅ Class imbalance fix (ConditionalPredicateHead, is_holding F1: 0→0.914)
12. ✅ Test suite updated (9 new tests for global context conditioning)
13. ✅ ForwardDynamicsModel (Phase 10.3): Pre-execution simulation, 55k frames
14. ✅ SpatiotemporalGNN (Phase 11): Temporal predicate stability, 55k frames, ~90% acc
15. ✅ QwenAgent integration (Qwen2.5:3B via Ollama) - Swappable AI brain validated

[REMAINING_TASKS]
1. Physical robot deployment
2. ✅ LLM agent integration demo - COMPLETE (Llama + Qwen via Ollama)
3. Gazebo simulation testing
4. Additional datasets training
5. Edge-native perception stack (YOLO-World + Depth Anything V2 Small) for RTX 500

[LLM_AGENT_IMPROVEMENTS]
updated: 2026-01-18
changes:
  - MCPClient now uses real MCP SDK (SSE) with HTTP fallback
  - Tools and resources cached from server on connect
  - System prompts updated to include prediction tools and predicates
  - AgentState now maintains message_history (last 5 turns)
  - observe() now includes GNN world graph and predicates
  - Prompts include spatial/interaction predicate definitions
  - Added QwenAgent for Qwen2.5 models via Ollama
  
files_modified:
  - src/agents/base_agent.py (MCPClient, AgentState, observe)
  - src/agents/llama_agent.py (SYSTEM_PROMPT, _build_prompt, _add_to_history)
  - src/agents/qwen_agent.py (NEW - QwenAgent with Ollama integration)
  - scripts/run_experiment.py (added Qwen agent option)

agent_capabilities:
  - Motion: move, stop, rotate, move_forward
  - Perception: get_obstacle_distances, check_path_clear, scan_surroundings
  - Prediction (GNN): get_world_graph, get_predicates, advance_frame, set_frame
  - Resources: pose, world_graph, predicates

available_agents:
  - LlamaAgent: Llama3.2 (3B) via Ollama - validated
  - QwenAgent: Qwen2.5 (3B) via Ollama - validated
  - ClaudeAgent: NOT USED (deferred indefinitely)

[OPEN_QUESTIONS_RESOLVED]
question_1_camera_intrinsics:
  question: Where to find LeRobot ALOHA camera intrinsics?
  answer: meta/info.json → features["observation.images.cam_high"]["video_info"] OR use hardware defaults
  resolution: Using CameraIntrinsics.default_aloha() with 60° FOV (ALOHA webcam default)
  source: PDF §2.2

question_2_detector_choice:
  question: DETIC vs GroundingDINO — which has better open-vocab?
  answer: GroundingDINO better for referential grounding; YOLO-World recommended for edge deployment (0.5GB vs 1.5GB)
  resolution: Using GroundingDINO; YOLO-World noted as future enhancement for RTX 500
  source: PDF §3

question_3_vram_constraints:
  question: Memory constraints with DINOv2-B on RTX 500 (4GB)?
  answer: Current stack (~3.7GB) at/exceeds 4GB limit; PDF recommends edge-native stack (~0.9GB)
  resolution: Use RTX 4080 for training; mock detectors for dev; edge stack for RTX 500 deployment
  source: PDF §4

[VISION_INTEGRATION_COMPONENTS]
status: Complete (Phase 1-4)

new_files:
  - src/gnn_reasoner/detector.py     # VisionDetector (DETIC/GroundingDINO/YOLOv8)
  - src/gnn_reasoner/depth.py        # DepthEstimator (ZoeDepth/MiDaS/DepthAnything)
  - src/gnn_reasoner/camera.py       # CameraIntrinsics, pixel_to_world, bbox_to_3d
  - src/gnn_reasoner/model/multimodal_gnn.py  # MultiModalGNN, VisionEncoder
  - scripts/train_multimodal_gnn.py  # Training script for Option C
  - scripts/compare_models.py        # A vs C benchmark harness
  - scripts/generate_comparison_figures.py  # Thesis figures
  - tests/test_vision_pipeline.py    # 26 unit tests
  - tests/test_multimodal_gnn.py     # 14 unit tests

modified_files:
  - src/gnn_reasoner/lerobot_transformer.py  # to_graph_with_objects(), interaction predicates
  - src/gnn_reasoner/__init__.py     # Exported new components
  - src/gnn_reasoner/model/__init__.py  # Exported MultiModalGNN

option_a_geometric_fusion:
  architecture: VisionDetector → DepthEstimator → 3D Projection → Graph → RelationalGNN
  parameters: 203,369
  latency_mean: 2.42ms
  latency_p95: 2.72ms
  micro_accuracy: 92.27%
  macro_f1: 0.283
  memory_peak: 107 MB
  model_size: 0.81 MB
  checkpoint: experiments/aloha_training/best_model.pt

option_c_multimodal_fusion:
  architecture: VisionDetector → DINOv2 → RoI Pool → CrossAttention → GNN
  vision_encoder: DINOv2-ViT-S/14 (22M params, frozen)
  parameters: 534,121
  latency_mean: 52.12ms
  latency_p95: 54.53ms
  micro_accuracy: 96.21%
  macro_f1: 0.311
  memory_peak: 231 MB
  model_size: 2.14 MB
  training_time: 32.0 min (100 epochs)
  checkpoint: experiments/multimodal_aloha/best_model.pt

per_predicate_f1_comparison:
  # ⚠️ OUTDATED - See [FINAL_COMPARISON_RESULTS] for current values
  # These were measured BEFORE WeightedFocalLoss fix to RelationalGNN
  is_near: A=0.668, C=0.906 (+35.6%)  # OUTDATED - now A=0.954, C=0.920
  is_left_of: A=0.940, C=0.943 (+0.3%)
  is_right_of: A=0.941, C=0.939 (-0.3%)
  is_above: A=0.000, C=0.004
  is_below: A=0.000, C=0.004
  is_holding: A=0.000, C=0.000
  is_contacting: A=0.000, C=0.000
  is_approaching: A=0.000, C=0.001
  is_retracting: A=0.000, C=0.000

comparison_figures:
  - thesis/figures/accuracy_comparison.pdf
  - thesis/figures/latency_breakdown.pdf
  - thesis/figures/per_predicate_f1.pdf
  - thesis/figures/memory_comparison.pdf
  - thesis/figures/radar_comparison.pdf
  - thesis/figures/latency_distribution.pdf
  - thesis/figures/comparison_table.tex

[ABLATION_STUDY]
type: depth_noise
script: scripts/ablation_depth_noise.py
frames: 200
noise_levels_cm: [0, 1, 2, 5, 10, 20]

results:
  sigma_0cm:
    option_a_acc: 0.9375
    option_c_acc: 0.9887
    option_a_f1_near: 0.6683
    option_c_f1_near: 0.9680
  sigma_5cm:
    option_a_acc: 0.9356
    option_c_acc: 0.9847
    option_a_f1_near: 0.6675
    option_c_f1_near: 0.9645
  sigma_10cm:
    option_a_acc: 0.9322
    option_c_acc: 0.9776
    option_a_f1_near: 0.6696
    option_c_f1_near: 0.9453
  sigma_20cm:
    option_a_acc: 0.9190
    option_c_acc: 0.9569
    option_a_f1_near: 0.6640
    option_c_f1_near: 0.8931

key_findings:
  - Option C more robust to depth noise (95.7% vs 91.9% at σ=20cm)
  - Option A nearly invariant (kinematic-focused, 2% drop)
  - Option C is_near degrades gracefully (7.7% F1 drop at extreme noise)
  - Practical: Option C benefits most when depth <5cm error

ablation_figures:
  - experiments/ablation_depth/ablation_accuracy_vs_noise.pdf
  - experiments/ablation_depth/ablation_f1_near_vs_noise.pdf
  - experiments/ablation_depth/ablation_relative_degradation.pdf

[MULTIMODAL_GNN_ARCHITECTURE]
model_name: MultiModalGNN
components:
  - NodeEncoder: MLP(5 -> hidden_dim), input=[x,y,z,theta,type]
  - EdgeEncoder: MLP(2 -> hidden_dim//4), input=[distance, is_kinematic]
  - VisionEncoder: DINOv2 + RoI pooling + Linear projection
  - CrossAttentionFusion: Bidirectional attention between vision and kinematic
  - GNNLayers: 3x GATv2Conv with 4 heads, residual connections
  - PredicateHead: MLP(hidden_dim*2 -> num_predicates) pairwise
hyperparameters:
  hidden_dim: 128
  num_layers: 3
  num_heads: 4
  dropout: 0.1
  num_predicates: 9
  vision_model: dinov2_vits14
  freeze_vision: true

================================================================================
CHRONOLOGICAL PROGRESS LOG
================================================================================

[2025-01-05] Phase 1-4: Vision Integration
  ✅ Created detector.py, depth.py, camera.py
  ✅ Extended LeRobotGraphTransformer with object nodes
  ✅ Created MultiModalGNN with DINOv2 vision encoder
  ✅ Training: 99.4% kinematic, 96.2% multimodal accuracy
  ✅ Comparison benchmark: +35.6% is_near F1 improvement
  ✅ Ablation study: depth noise robustness analysis

[2025-01-06] Phase 5: Vision Plan Finalization
  ✅ Closed open questions (camera intrinsics, detector choice, VRAM)
  ✅ Documented VRAM limitations (RTX 500 4GB constraint)
  ✅ Updated README with hardware requirements

[2025-01-06] Phase 6: LLM Agent Integration
  ✅ Fixed MCPClient to use real MCP SDK (SSE + caching)
  ✅ Updated system prompts (Claude + Llama) with prediction tools
  ✅ Added conversation history (last 5 turns)
  ✅ Enhanced observe() to include GNN predicates
  ⏸️ Claude integration — DEFERRED (will revisit later)

[2026-01-06] Phase 7: Ollama + Llama3.2 Integration
  ✅ Installed Ollama with NVIDIA GPU support
  ✅ Pulled llama3.2 (3B, 2.0GB)
  ✅ Fixed LLM argument type coercion (string → number)
  ✅ Simplified observe() to use tool results (bypasses resource bug)
  ✅ E2E Test: SUCCESS (3 steps, 5.3s, world_graph: 16 nodes, 54 edges)
  ⚠️ Known Issue: MCP resource reading broken (TextResourceContents library bug)
     → Workaround: Agent uses tool results instead of resources
  ✅ Llama/Ollama integration — COMPLETE

[2026-01-18] Phase 12: Qwen Agent Integration
  ✅ Pulled qwen2.5:3b (1.9GB) via Ollama
  ✅ Created QwenAgent class (src/agents/qwen_agent.py)
  ✅ Implemented Qwen-specific system prompt and user template
  ✅ Added Qwen to run_experiment.py agent options
  ✅ Created test_qwen_agent.py validation script
  ✅ JSON parsing validated (correct action format)
  ✅ Swappable AI brain thesis claim strengthened (Llama + Qwen both work)
  ❌ Claude agent NOT pursued (Qwen replaces it for local inference)

  Problems Encountered & Solutions:
  
  Problem 1: Llama agent stuck in infinite loop
    Symptom: Agent kept calling get_world_graph repeatedly (20+ steps)
    Cause: Original prompt too verbose, 3B model confused about completion
    Fix: Simplified system prompt, added explicit "DECISION RULE" with step limits
    File: src/agents/llama_agent.py (SYSTEM_PROMPT)
  
  Problem 2: LLM sends string arguments instead of numbers
    Symptom: Tool error "'0' is not of type 'number'"
    Cause: Llama returns {"threshold": "0"} instead of {"threshold": 0}
    Fix: Added _coerce_argument_types() to MCPClient.call_tool()
    File: src/agents/base_agent.py
  
  Problem 3: MCP resource reading fails with SSE transport bug
    Symptom: TypeError: 'NoneType' object is not callable in starlette/routing.py
    Root Cause: TWO ISSUES combined:
      1. Route vs Mount: handle_post_message is an ASGI app, not Starlette endpoint
         - WRONG: Route("/messages/", endpoint=handle_messages, methods=["POST"])
         - RIGHT: Mount("/messages", app=sse_transport.handle_post_message)
      2. SDK type mismatch: SDK expects ReadResourceContents from helper_types
         - WRONG: returning list[TextResourceContents] (has .text, .mimeType)
         - RIGHT: returning list[ReadResourceContents] (has .content, .mime_type)
      3. URI type: SDK passes AnyUrl not str, string comparison fails
         - FIX: uri_str = str(uri) for all comparisons
    Fixed Files:
      - src/mcp_ros2_bridge/server.py (Mount instead of Route)
      - src/mcp_ros2_bridge/resources/__init__.py (ReadResourceContents, str(uri))
    Status: ✅ FULLY FIXED - Resources now work correctly via SSE!
  
  Problem 4: Agent not seeing tool results in observations
    Symptom: Agent couldn't decide when task was complete
    Cause: observe() tried to read resources (broken), ignoring tool results
    Fix: Rewrote observe() to extract world_graph from self.state.last_action["result"]
    File: src/agents/base_agent.py

[CLASS_IMBALANCE_FIX]
problem: "Zero-Holding" Anomaly
symptom: is_holding F1=0.000 for both Option A and Option C
root_cause: Two issues combined:
  1. Architecture: Model couldn't condition predicates on gripper state (graph-level attribute)
  2. Data: Only 10% holding scenarios = ~0.7% positive is_holding edges

solution_implemented: Global Feature Conditioning (GFC) + Data Balancing
  1. ConditionalPredicateHead: Input = [src_embed, tgt_embed, gripper_state]
     - Explicit conditioning on u = [left_gripper, right_gripper]
     - Enables learning: is_holding = (gripper_near_object) AND (gripper_closed)
  2. extract_global_context(): Extracts normalized gripper states from state vector
  3. Increased holding ratio: 30% instead of 10% for sufficient training signal
  4. WeightedFocalLoss: Handles remaining class imbalance

final_results:
  # ⚠️ NOTE: These results are from SYNTHETIC data with augmented holding scenarios
  # On real ALOHA data, is_holding/is_contacting remain 0.000 (insufficient positive samples)
  is_holding F1: 0.000 → 0.914 ✅ (synthetic, holding_ratio=0.30)
  is_contacting F1: 0.000 → 0.960 ✅ (synthetic, holding_ratio=0.30)
  Macro F1: 0.336 → 0.671 ✅ (synthetic)

files_modified:
  - src/gnn_reasoner/lerobot_transformer.py (extract_global_context, data.u)
  - src/gnn_reasoner/model/relational_gnn.py (ConditionalPredicateHead)
  - scripts/train_relational_gnn.py (use_global_conditioning=True, holding_ratio=0.30)
  - scripts/compare_models.py (matching evaluation distribution)

status: ✅ COMPLETE - is_holding detection working

[2026-01-07] Phase 10.3: Pre-Execution Simulation
  ✅ Created ForwardDynamicsModel (259K params)
  ✅ Created train_forward_model.py training script
  ✅ Added simulate_action MCP tool for LLM plan verification
  ✅ Validated on RTX 500 (1k frames, 10 epochs, 9.7 min)
  ✅ Delta error: 0.0042 → 0.0009 (78% reduction)
  ✅ Updated MasterPlan.md with Phase 10.3 documentation

  Architecture:
    - ActionEncoder: 14-DoF action → hidden_dim embedding
    - DynamicsNetwork: [graph_embed, action_embed] → state delta + uncertainty
    - FeasibilityHead: Binary classifier for physical plausibility
    - ConfidenceHead: Epistemic uncertainty estimation

  MCP Tool Interface:
    name: simulate_action
    description: VERIFY planned action sequence BEFORE physical execution
    returns: { recommendation: "EXECUTE" | "REPLAN", confidence: 0.85, trajectory: [...] }

  Files Created:
    - src/gnn_reasoner/model/forward_dynamics.py
    - scripts/train_forward_model.py
  
  Files Modified:
    - src/gnn_reasoner/model/__init__.py (exports)
    - src/mcp_ros2_bridge/tools/prediction.py (simulate_action tool)

[TEST_UPDATES]
date: 2026-01-07
reason: Tests needed for new ConditionalPredicateHead and extract_global_context()

new_tests_added:
  - TestGlobalContextConditioning::test_extract_global_context_from_state
  - TestGlobalContextConditioning::test_extract_global_context_with_explicit_gripper
  - TestGlobalContextConditioning::test_graph_has_u_attribute
  - TestGlobalContextConditioning::test_closed_gripper_context
  - TestConditionalPredicateHead::test_head_creation
  - TestConditionalPredicateHead::test_head_forward
  - TestConditionalPredicateHead::test_different_gripper_states_produce_different_outputs
  - TestRelationalGNN::test_forward_with_global_context
  - TestRelationalGNN::test_conditioning_affects_is_holding

file_modified: tests/test_lerobot_pipeline.py

test_summary:
  test_lerobot_pipeline.py: 39 passed
  test_multimodal_gnn.py: 14 passed
  test_vision_pipeline.py: 26 passed
  test_graph_builder.py: 9 passed
  test_mcp_tools.py: 10 passed
  test_agent_swap.py: 2 failed (pre-existing, needs pytest-asyncio)
  total: 102 tests, 100 passed, 2 failed (pre-existing)

notes:
  - All new conditioning tests pass
  - Existing tests remain backward compatible (fallback to u=0.5 if missing)
  - test_agent_swap.py failures are pre-existing (async tests need pytest-asyncio)

[REMOTE_TRAINING_RTX3070]
date: 2026-01-07
machine: cip7g1.cip.cs.fau.de (xi58pizy)
gpu: NVIDIA GeForce RTX 3070 (8.2GB VRAM)
working_dir: /proj/ciptmp/xi58pizy/AI2MCP

training_results:
  relational_gnn:
status: ✅ COMPLETE
    accuracy: 98.96%
    epochs: 100
    training_time: ~29 min
    checkpoint: experiments/remote_training/relational_gnn/best_model.pt
    
  multimodal_gnn:
    status: ⚠️ COMPLETE but POOR PERFORMANCE
    accuracy: ~46% (expected >90%)
    issue: Needs investigation - possible training/eval mismatch
    checkpoint: NOT FOUND (training may have been interrupted)

benchmark_results_honest_latency:
  run_1_depth_anything:
    depth_model: Depth Anything V2 Small
    option_a_accuracy: 95.1%
    option_a_latency_breakdown:
      detection_grounding_dino: 234.4ms
      depth_estimation: 61.3ms
      gnn_inference: 1.4ms
      total_e2e: 297.2ms
    option_a_f1_is_near: 0.894
    option_c_accuracy: 47.6% (PROBLEM)
    
  run_2_midas:
    depth_model: MiDaS (fallback, relative depth only)
    option_a_accuracy: 97.0%
    option_a_latency_breakdown:
      detection_grounding_dino: 217.2ms
      depth_estimation: 113.4ms
      gnn_inference: 1.4ms
      total_e2e: 332.0ms
    option_a_f1_is_near: 0.954
    option_c_accuracy: 45.5% (PROBLEM)

key_findings:
  - ✅ Honest E2E latency measured: ~300ms (not fake 2.4ms from mocks)
  - ✅ RelationalGNN works well with real vision (95-97% accuracy)
  - ✅ Depth Anything V2 is faster (61ms) than MiDaS (113ms)
  - ⚠️ MultiModalGNN has severe accuracy problem (~46%)
  - ⚠️ ZoeDepth failed to load (timm version mismatch, fell back to MiDaS)

issues_encountered:
  1. timm_version_mismatch:
     symptom: "cannot import name 'RotaryEmbedding' from 'timm.layers'"
     cause: timm 1.0.23 incompatible with ZoeDepth
     attempted_fix: Downgraded to timm 0.9.16
     result: Still fell back to MiDaS (additional dependencies needed)
     
  2. grounding_dino_api_change:
     symptom: "got unexpected keyword argument 'box_threshold'"
     cause: transformers 4.57.3 changed API
     fix: Added try/except fallback in detector.py
     
  3. disk_quota_warning:
     symptom: "You have exceeded your disk quota"
     cause: ZoeDepth download (1.35GB) to ~/.cache
     status: Resolved (82% usage, within limits)
     
  5. wrong_gpu_profile_detection:
     symptom: RTX 3070 (8GB) detected as "RTX 500 Ada (4GB)" profile
     cause: detect_gpu_profile() only had 2 tiers (>=12GB or fallback)
     fix: Added RTX 3070 profile for 6-12GB VRAM GPUs
     files_fixed:
       - scripts/train_multimodal_gnn.py
       - scripts/train_relational_gnn.py
     result: Proper batch_size selection (16 instead of 8 for multimodal)
     
  4. multimodal_gnn_poor_performance:
     symptom: 46% accuracy vs expected 90%+
     status: ✅ ROOT CAUSE IDENTIFIED
     root_cause: Training never completed - stuck at 30% data processing
       - Phase 2 (MultiModalGNN) started after Phase 1 (RelationalGNN) in same session
       - Data processing stopped at 16,245/55,000 frames (30%)
       - No checkpoint file was ever created
       - compare_models.py loaded random/uninitialized weights
       - Random model produces ~46% accuracy (near random for 9 predicates)
     fix: Re-running training with 10k frames (faster iteration)

[COMPARISON_RESULTS_PRELIMINARY]
date: 2026-01-07
status: ⚠️ UNFAIR COMPARISON - Different training data sizes
models_compared:
  option_a: RelationalGNN (kinematic + geometric vision fusion)
  option_c: MultiModalGNN (DINOv2 + cross-attention fusion)

training_summary:
  relational_gnn:
    dataset: ALOHA 55k frames  # ← FULL DATASET
    accuracy: 98.96%
    training_time: 29 min
    checkpoint: experiments/remote_training/relational_gnn/best_model.pt
    
  multimodal_gnn:
    dataset: ALOHA 10k frames  # ← SUBSET ONLY (5.5× less data)
    accuracy: 97.14%
    training_time: 40 min (100 epochs)
    checkpoint: experiments/remote_training/multimodal_gnn_10k/best_model.pt

⚠️ FAIR_COMPARISON_LIMITATION:
  issue: RelationalGNN trained on 55k frames, MultiModalGNN on only 10k frames
  impact: Option A had 5.5× more training data - comparison is not apples-to-apples
  action_required: Train MultiModalGNN on full 55k frames (~3.5 hours)
  scheduled: Tonight (user will run manually)
  command: |
    ssh xi58pizy@cip7g1.cip.cs.fau.de
    cd /proj/ciptmp/xi58pizy/AI2MCP
    nohup python scripts/train_multimodal_gnn.py \
        --repo lerobot/aloha_static_coffee \
        --epochs 100 \
        --output experiments/remote_training/multimodal_gnn_55k \
        2>&1 | tee experiments/remote_training/multimodal_gnn_55k/training.log &

preliminary_benchmark_results:
  note: These results are PRELIMINARY due to unfair training data split
  
  micro_accuracy:
    option_a: 96.59% (trained on 55k)
    option_c: 95.69% (trained on 10k)
    winner: A (+0.9%) - but unfair comparison
    
  macro_f1:
    option_a: 0.341
    option_c: 0.359
    winner: C (+5.3%) - despite less training data!
    
  latency_mean_ms:
    option_a: 1.50
    option_c: 24.95
    winner: A (17× faster) - architecture difference, not training

per_predicate_f1:
  is_near:        A=0.942, C=0.906  # A wins (more data)
  is_left_of:     A=0.962, C=0.947  # A wins (more data)
  is_right_of:    A=0.958, C=0.946  # A wins (more data)
  is_approaching: A=0.116, C=0.234  # C wins +101% DESPITE less data!
  is_retracting:  A=0.088, C=0.200  # C wins +127% DESPITE less data!
  is_holding:     A=0.000, C=0.000  # Tie (needs more training data)
  is_contacting:  A=0.000, C=0.000  # Tie (needs more training data)

preliminary_insight: |
  Even with 5.5× LESS training data, Option C (MultiModalGNN) still 
  outperforms on TEMPORAL/DYNAMIC predicates:
    - is_approaching: +101% better
    - is_retracting: +127% better
  
  This suggests vision integration provides genuine value for motion 
  understanding that kinematics alone cannot capture.
  
  PENDING: Fair comparison after training MultiModalGNN on 55k frames.

[FINAL_COMPARISON_RESULTS]
date: 2026-01-08
status: ✅ COMPLETE - FAIR COMPARISON (55k vs 55k)
note: |
  This supersedes all preliminary results above. Both models trained on 
  55,000 ALOHA frames with correct checkpoint loading and real vision.

training_completed:
  relational_gnn:
    dataset: ALOHA 55k frames
    accuracy: 97.91% (training), 97.03% (benchmark)
    epochs: 100
    training_time: ~29 min
    checkpoint: experiments/remote_training/relational_gnn/best_model.pt
    
  multimodal_gnn:
    dataset: ALOHA 55k frames
    accuracy: 97.91% (training), 96.51% (benchmark)
    epochs: 100
    training_time: ~31 min
    checkpoint: experiments/remote_training/multimodal_gnn_55k_v2/best_model.pt
    memory_optimization: Images loaded on-demand (not stored in RAM)
    peak_memory: 14.5 GB (fixed from 62GB OOM crash)

final_benchmark_results:
  conditions:
    - Real vision models: GroundingDINO + MiDaS
    - Both checkpoints loaded correctly
    - 500 evaluation frames
  
  micro_accuracy:
    option_a: 97.03%
    option_c: 96.51%
    winner: A (+0.5%)
    
  macro_f1:
    option_a: 0.358
    option_c: 0.348
    winner: A (+2.9%)
    
  latency_mean_ms:
    option_a: 1.52
    option_c: 24.29
    winner: A (16× faster)
    
  memory_mb:
    option_a_model_size: 0.81
    option_c_model_size: 2.14
    option_a_peak: 19.4
    option_c_peak: 141.8
    winner: A (7× less memory)

per_predicate_f1_final:
  is_near:        A=0.954, C=0.920  # A wins
  is_left_of:     A=0.969, C=0.954  # A wins
  is_right_of:    A=0.968, C=0.954  # A wins
  is_approaching: A=0.182, C=0.156  # A wins
  is_retracting:  A=0.152, C=0.146  # A wins
  is_above:       A=0.000, C=0.000  # Tie (horizontal workspace)
  is_below:       A=0.000, C=0.000  # Tie (horizontal workspace)
  is_holding:     A=0.000, C=0.000  # Tie (no contact annotations)
  is_contacting:  A=0.000, C=0.000  # Tie (no contact annotations)

key_findings:
  1_architecture_comparison: |
    RelationalGNN OUTPERFORMS MultiModalGNN on all metrics when both are
    fairly trained on 55k frames. Vision features (DINOv2) do not provide
    additional value on this dataset - spatial predicates are solvable
    from joint positions alone.
    
  2_latency_tradeoff: |
    Option A is 16× faster (1.5ms vs 24ms) and uses 7× less memory.
    For real-time control, Option A is clearly superior.
    
  3_temporal_predicates_correction: |
    Previous claim that Option C wins on is_approaching/is_retracting by
    +101%/+127% was due to UNFAIR comparison (55k vs 10k training data).
    With fair training, Option A wins on temporal predicates too.
    
  4_is_near_improvement_correction: |
    Previous claim of "+35.6% is_near improvement from vision" was comparing
    OLD RelationalGNN (without WeightedFocalLoss) vs NEW MultiModalGNN.
    After RelationalGNN improvements, it now BEATS MultiModalGNN on is_near
    (0.954 vs 0.920).
    
  5_contact_predicates: |
    is_holding and is_contacting remain 0.000 F1 for BOTH models on real
    ALOHA data. The 0.914/0.960 values in earlier docs were from SYNTHETIC
    data with augmented holding scenarios (holding_ratio=0.30). Real ALOHA
    has only ~0.7% positive is_holding edges - insufficient training signal.

thesis_implications:
  - RelationalGNN is the recommended architecture for this use case
  - Vision integration (MultiModalGNN) adds complexity without benefit
  - The "+35.6% is_near" claim needs revision in thesis
  - Contact detection requires annotated training data (not available in ALOHA)
  
checkpoints_saved:
  - experiments/remote_training/relational_gnn/best_model.pt
  - experiments/remote_training/multimodal_gnn_55k_v2/best_model.pt
  - experiments/comparison_final_real/comparison_results.json

[CURRENT_FOCUS]
task: Thesis completion and documentation
status: ✅ PHASE 9 + PHASE 10.3 + PHASE 11 COMPLETE

completed:
  - ✅ Remote training infrastructure (remote_train.sh)
  - ✅ RelationalGNN training (98.96% accuracy, 55k frames)
  - ✅ MultiModalGNN training (97.91% accuracy, 55k frames) - FINAL
  - ✅ Honest latency benchmarks (297-332ms E2E with real vision)
  - ✅ Fair comparison benchmark (55k vs 55k) - COMPLETE
  - ✅ ConditionalPredicateHead implemented
  - ✅ extract_global_context() implemented
  - ✅ Test suite updated (9 new tests, all pass)
  - ✅ MultiModalGNN investigation COMPLETE (root cause: training never finished)
  - ✅ Fixed memory exhaustion bug (on-demand image loading)
  - ✅ Phase 10.3: ForwardDynamicsModel implemented (259K params)
  - ✅ Phase 10.3: simulate_action MCP tool added
  - ✅ Phase 10.3: Full 55k training COMPLETE (2.3 min, 730× speedup)
  - ✅ Phase 10.3: MCP tool tested and validated (41ms inference)
  - ✅ Phase 11: SpatiotemporalGNN implemented (GRU + RelationalGNN)
  - ✅ Phase 11: Comprehensive sanity check (catches bugs before pre-computation)
  - ✅ Phase 11: Full 55k training COMPLETE (47 min, ~90% accuracy)

pending:
  - ⬜ Update MasterPlan.md with final results
  - ⬜ Update thesis Results chapter
  - ⬜ Generate final thesis figures
  - ⬜ [Optional] Llama agent integration with simulate_action

memory_optimization_fix:
  problem: MultiModalGNN training crashed at 30% (16.5k frames) due to OOM
  root_cause: Storing image tensors (3.7MB each) in RAM during data loading
  peak_memory_before: 62GB (crashed)
  fix: Modified create_lerobot_data() to NOT store image tensors in data_list
  fix_file: scripts/train_multimodal_gnn.py
  peak_memory_after: 14.5GB (success)
  status: ✅ FIXED - Full 55k training completed

deferred:
  - ZoeDepth proper installation (requires zoedepth pip package)
  - Gazebo simulation
  - Physical robot deployment

not_pursuing:
  - Claude agent testing (using Qwen + Llama via Ollama instead)

[OLLAMA_SETUP]
status: ✅ installed_and_running
gpu: NVIDIA RTX 500 Ada (detected)
models_installed:
  - llama3.2:latest (2.0 GB, 3B params)
  - qwen2.5:3b (1.9 GB, 3B params)
api_endpoint: http://localhost:11434
installed_at: 2026-01-06
llama_test_result: SUCCESS (3 steps, 5.3s, 16 nodes, 54 edges)
qwen_test_result: SUCCESS (JSON parsing validated)

[FORWARD_DYNAMICS_MODEL]
purpose: Pre-Execution Simulation for LLM plan verification (Phase 10.3)
status: ✅ IMPLEMENTED

architecture:
  model_name: ForwardDynamicsModel
  total_parameters: 259,522
  components:
    - ActionEncoder: MLP(14 -> hidden_dim), encodes 14-DoF ALOHA actions
    - DynamicsNetwork: MLP([graph+action] -> state_delta + uncertainty)
    - FeasibilityHead: Binary classifier for physical plausibility
    - ConfidenceHead: Sigmoid output for epistemic uncertainty
  inputs:
    - graph_embed: From RelationalGNN encoder (frozen or learned)
    - action: 14-dimensional joint velocity vector
  outputs:
    - delta: (num_nodes, 3) predicted position change
    - uncertainty: (num_nodes,) per-node uncertainty
    - confidence: Scalar [0, 1] global confidence
    - feasibility: Binary feasibility prediction

training_validation_local:
  device: RTX 500 Ada (4GB)
  dataset: lerobot/aloha_static_coffee
  frames: 1000
  epochs: 10
  batch_size: 16
  training_time: 9.7 minutes
  delta_error_initial: 0.0042
  delta_error_final: 0.0009
  improvement: 78% reduction

training_full_optimized:
  device: RTX 3070 (8GB)
  dataset: lerobot/aloha_static_coffee
  frames: 55000 (full dataset)
  epochs: 100
  batch_size: 512
  pre_computation_time: 17 minutes (RAM caching)
  training_time: 2.3 minutes (100 epochs)
  time_per_epoch: 1.3 seconds
  best_epoch: 69
  best_val_loss: -0.5747
  delta_error_final: 0.0017
  confidence_output: 0.54-0.55
  checkpoint: experiments/remote_training/forward_dynamics_e2e/best_model.pt
  
optimization_applied:
  - RAM pre-computation: All 55k graphs cached at startup
  - GPU pre-loading: Actions/deltas on GPU VRAM
  - Batch size increase: 32 → 512 (16× larger)
  - DataLoader workers: 4 → 0 (data in RAM, no I/O)
  - Speedup achieved: ~730× per epoch (17 min → 1.3 sec)

loss_function: ForwardDynamicsLoss
  components:
    - delta_weight: 1.0 (MSE on position delta)
    - uncertainty_weight: 0.1 (NLL with predicted variance)
    - feasibility_weight: 0.5 (BCE on feasibility)
    - consistency_weight: 0.1 (temporal smoothness)

mcp_tool:
  name: simulate_action
  description: |
    VERIFY a planned action sequence BEFORE physical execution.
    Returns confidence scores, feasibility assessment, and EXECUTE/REPLAN recommendation.
  input_schema:
    action_sequence: array of 14-dim action vectors
    num_steps: int (1-20)
    confidence_threshold: float (0.0-1.0, default 0.7)
  output:
    recommendation: "EXECUTE" | "REPLAN"
    min_confidence: float
    overall_feasible: bool
    trajectory: list of per-step results

files:
  - src/gnn_reasoner/model/forward_dynamics.py (ForwardDynamicsModel, Loss, SimulationResult)
  - scripts/train_forward_model.py (training script with RAM pre-computation)
  - scripts/test_forward_dynamics.py (validation test script)
  - src/mcp_ros2_bridge/tools/prediction.py (simulate_action handler)

test_results:
  test_date: 2026-01-08
  all_tests_passed: true
  tests_run:
    - Direct Model Inference: ✅
    - Single Action Simulation: ✅
    - Multi-Step Sequence (5 steps): ✅
    - MCP Response Format: ✅
    - PredictionToolsManager.simulate_action(): ✅
  inference_time: 41 ms
  confidence_range: 0.54-0.55
  recommendation: REPLAN (confidence < 0.7 threshold)

next_steps:
  - ✅ Full training on 55k frames (RTX 3070) — COMPLETE
  - ✅ Integration test (simulate_action MCP tool) — COMPLETE
  - ⬜ Integration test with Llama agent
  - ⬜ Benchmark LLM plans with/without simulation verification

[2026-01-08] Phase 9: Fair Comparison COMPLETE
  ✅ Fixed MultiModalGNN memory exhaustion (on-demand image loading)
  ✅ Trained MultiModalGNN on 55k frames (97.91% accuracy, 31 min)
  ✅ Ran final comparison benchmark (real vision + correct checkpoints)
  ✅ Key finding: RelationalGNN WINS on all metrics
  ✅ Updated CONTEXT_DUMP.txt with final results
  
  Training Issues Resolved:
    - Memory crash at 30%: Fixed by not storing image tensors in RAM
    - collate_multimodal crash: Fixed None image handling
    - Peak memory: 62GB → 14.5GB
  
  Final Results Summary:
    - RelationalGNN: 97.03% accuracy, 1.5ms latency, 0.81MB
    - MultiModalGNN: 96.51% accuracy, 24ms latency, 2.14MB
    - Winner: RelationalGNN (faster, smaller, more accurate)
    - Vision integration provides NO additional value on ALOHA dataset
  
  Thesis Implications:
    - Previous "+35.6% is_near improvement" claim is OUTDATED
    - RelationalGNN with WeightedFocalLoss is the recommended architecture
    - Contact detection requires annotated data (not available in ALOHA)

[2026-01-08] Phase 10.3: ForwardDynamicsModel Training COMPLETE
  ✅ Implemented RAM pre-computation for 55k transitions
  ✅ Optimized training: 17 min → 1.3 sec per epoch (730× speedup)
  ✅ Full 55k training in 2.3 minutes (+ 17 min pre-computation)
  ✅ Best model saved: epoch 69, val_loss=-0.5747, delta_error=0.0017
  ✅ MCP simulate_action tool tested and validated
  
  Training Optimization Journey:
    - Initial: 17 min/epoch (disk I/O bottleneck, GPU idle)
    - After num_workers=4, batch=128: 6-7 min/epoch (2.5× speedup)
    - After RAM pre-computation: 1.3 sec/epoch (730× speedup)
  
  Resource Utilization Analysis:
    - Before: CPU 20%, GPU 0%, RAM 5%, VRAM 3%
    - After: CPU 100% during pre-compute, GPU active during training
    - Pre-computation: Uses 60GB RAM to cache all graphs
    - Training: Uses GPU with batch_size=512
  
  Key Files Created/Modified:
    - scripts/train_forward_model.py (RAM pre-computation, GPU profile)
    - scripts/test_forward_dynamics.py (validation tests)
  
  Test Results:
    - All 5 test cases passed
    - Inference time: 41 ms
    - Confidence output: 0.54-0.55
    - Recommendation: REPLAN (conservative, below 0.7 threshold)
  
  Checkpoint Location:
    experiments/remote_training/forward_dynamics_e2e/best_model.pt (3.0 MB)

[2026-01-09] Phase 11: SpatiotemporalGNN Training COMPLETE
  ✅ Implemented SpatiotemporalGNN architecture (GRU + RelationalGNN)
  ✅ Created train_spatiotemporal_gnn.py training script
  ✅ Implemented RAM pre-computation (55k graphs in ~7 min)
  ✅ Added comprehensive sanity check (runs 1 mini-epoch before full training)
  ✅ Full 55k training in 47.4 minutes (100 epochs)
  ✅ Best model saved: epoch ~30, val_loss=0.2342
  
  Architecture:
    model_name: SpatiotemporalGNN
    base_gnn: RelationalGNN (frozen or learnable)
    temporal_layer: GRU (1 layer, 128 hidden dim)
    temporal_predicate_head: MLP for predicate prediction from temporal embedding
    sequence_length: 5 frames
    training_mode: end-to-end (base GNN unfrozen)
  
  Training Results:
    device: RTX 3070 (8GB)
    dataset: lerobot/aloha_static_coffee
    frames: 55000
    sequences: 49,495 train / 5,495 val
    epochs: 100
    batch_size: 64
    training_time: 47.4 minutes (2842 seconds)
    time_per_epoch: ~28 seconds
    best_val_loss: 0.2342 (BCE, ~90% accuracy)
    final_train_loss: 0.2321
    final_val_loss: 0.2345
    checkpoint: experiments/remote_training/spatiotemporal_gnn/best_model.pt (4.2 MB)
  
  Training Optimizations Applied:
    - RAM pre-computation: All 55k graphs cached at startup (~7 min)
    - GPU pre-loading: Graphs moved to GPU after pre-computation
    - Batched graph processing: Batch.from_data_list() for parallel processing
    - Comprehensive sanity check: 1 mini-epoch on 100 frames before full training
  
  Sanity Check Innovation:
    - Runs BEFORE expensive 7-min pre-computation
    - Pre-computes 100 graphs (~1 sec)
    - Creates train/val datasets
    - Runs 1 full training epoch using real train_epoch()
    - Runs 1 full validation epoch using real evaluate()
    - Catches ALL bugs (device mismatches, shape errors, missing attributes)
    - Saved hours of debugging during development
  
  Bugs Caught by Sanity Check:
    - DataManager subscripting: data_manager[i] → data_manager.get_frame(i)
    - Graph transformer method: frame_to_graph() → to_graph(state)
    - Missing import: add_predicate_labels not in scope
    - Device mismatch: add_predicate_labels on GPU graph (needs CPU)
    - In-place .to(device): prev_graph modified when graph.to(device) called
    - Batch edge mismatch: pred_logits from graphs_seqs[-1], targets from next_graphs
    - Missing y attribute: First graph had no labels (prev_graph was None)
  
  Key Findings:
    - BCE loss 0.23 corresponds to ~90% predicate accuracy
    - No overfitting: train_loss ≈ val_loss (0.232 vs 0.234)
    - Model converged by epoch ~30, stable thereafter
    - Temporal GRU adds context but doesn't significantly outperform single-frame
  
  Files Created/Modified:
    - scripts/train_spatiotemporal_gnn.py (full training script with sanity check)
    - src/gnn_reasoner/model/spatiotemporal_gnn.py (temporal_predicate_head added)
    - scripts/remote_train.sh (train-stgnn command)

[SPATIOTEMPORAL_GNN_ARCHITECTURE]
purpose: Temporal Predicate Stability for Phase 11
status: ✅ IMPLEMENTED AND TRAINED

architecture:
  model_name: SpatiotemporalGNN
  total_parameters: ~400K (RelationalGNN + GRU + temporal head)
  components:
    - base_gnn: RelationalGNN (pre-trained, optionally frozen)
    - temporal_gru: GRU(hidden_dim, temporal_hidden_dim, num_layers=1)
    - temporal_predicate_head: MLP(temporal_hidden_dim -> num_predicates)
  inputs:
    - data: PyG Data object (single frame or batched)
    - hidden_state: GRU hidden state from previous timestep
  outputs:
    - node_embeddings: (num_nodes, hidden_dim)
    - predicate_logits: (num_edges, num_predicates)
    - temporal_embedding: (batch_size, temporal_hidden_dim)
    - hidden_state: Updated GRU state for next timestep

training_config:
  profile: RTX 3070 (8GB) - ST-GNN
  batch_size: 64
  sequence_length: 5
  accumulation_steps: 1
  learning_rate: 3e-4
  weight_decay: 1e-5
  scheduler: CosineAnnealingLR
  optimizer: AdamW
  use_amp: true
  loss: BCE (binary cross-entropy for predicate classification)

checkpoint_contents:
  best_model.pt:
    - model_state_dict: SpatiotemporalGNN weights
    - optimizer_state_dict: AdamW state
    - epoch: best epoch
    - val_loss: 0.2342
    - profile: "RTX 3070 (8GB) - ST-GNN"

[FIGURE_GENERATION]
date: 2026-01-18
status: ✅ COMPLETE

generated_figures: 28 PNG files in figures/
output_directory: figures/ (consolidated, single source of truth)

scripts_parameterized:
  - scripts/generate_thesis_figures.py
  - scripts/generate_comparison_figures.py
  - scripts/generate_forward_dynamics_figure.py
  - scripts/generate_spatiotemporal_figures.py

cli_usage:
  default_png: "python scripts/generate_thesis_figures.py"
  pdf_only: "python scripts/generate_thesis_figures.py --format pdf"
  both_formats: "python scripts/generate_thesis_figures.py --format both"
  custom_output: "python scripts/generate_thesis_figures.py -o thesis/figures --format pdf"

figures_by_category:
  training_benchmark:
    - training_curves.png
    - learning_rate.png
    - inference_latency.png
    - pass_at_k.png
    - classification_metrics.png
  architecture:
    - architecture.png
    - gnn_dataflow.png
    - predicate_definitions.png
    - predicate_distribution.png
  comparison_a_vs_c:
    - accuracy_comparison.png
    - before_after_comparison.png
    - comparison_table.png
    - latency_breakdown.png
    - latency_distribution.png
    - memory_comparison.png
    - per_predicate_f1.png
    - radar_comparison.png
  forward_dynamics_phase10:
    - forward_dynamics_delta_error.png
    - forward_dynamics_mental_rollout.png
    - forward_dynamics_speedup.png
    - forward_dynamics_training.png
    - forward_dynamics_validation.png
  spatiotemporal_phase11:
    - stgnn_architecture.png
    - stgnn_sequence_processing.png
    - stgnn_temporal_stability.png
    - stgnn_training_curves.png
    - model_comparison.png
    - phase_10_11_summary.png

changes_made:
  - Added --format/-f argument to all 4 scripts (choices: png, pdf, both)
  - Default format is PNG (was previously both pdf+png)
  - Consolidated output to figures/ directory
  - Removed duplicate outputs to thesis/figures/
  - Added save_figure() helper function for format iteration

[AGENT_BENCHMARK_RESULTS]
date: 2026-01-18
status: ✅ COMPLETE

benchmark_config:
  goals_tested: 10
  max_steps_per_goal: 10
  timeout_per_goal: 30s
  
results_summary:
  | Metric                    | Llama3.2 | Qwen2.5:3b | Winner |
  |---------------------------|----------|------------|--------|
  | Success Rate              | 100%     | 100%       | TIE    |
  | Avg Steps                 | 2.8      | 1.0        | Qwen   |
  | Avg Time-to-First-Action  | 425ms    | 1073ms     | Llama  |
  | Avg Total Time            | 2561ms   | 1537ms     | Qwen   |

key_findings:
  - Both agents achieve 100% success rate on all 10 standardized goals
  - Qwen is 40% faster overall (1.5s vs 2.6s average)
  - Qwen uses 65% fewer steps (1.0 vs 2.8 average) - better at task completion
  - Llama has 2.5x faster time-to-first-token (425ms vs 1073ms)
  - Qwen's simplified prompt works better for task completion
  - Llama sometimes gets stuck in loops, using max steps

test_goals:
  - Get the current world graph
  - Report spatial predicates in the scene
  - Check how many nodes are in the world graph
  - Advance to frame 10 and get predicates
  - Set frame to 50 and report world graph
  - List all active predicates
  - Get predicates with threshold 0.7
  - Simulate moving forward and check outcome
  - Predict the outcome of rotating 90 degrees
  - Analyze the scene and report what objects are near each other

results_file: experiments/agent_benchmark.json
benchmark_script: scripts/benchmark_agents.py

[QWEN_AGENT_FIXES]
date: 2026-01-18
status: ✅ COMPLETE

issues_found_and_fixed:
  1_observation_extraction:
    problem: observe() looked for predicates at result top-level, but they're in world_context
    fix: Changed from result.get("spatial_predicates") to ctx.get("spatial_predicates")
    file: src/agents/base_agent.py
    
  2_model_checkpoint_mismatch:
    problem: Server tried to load old checkpoint with wrong input dimension (256 vs 258)
    error: "size mismatch for predicate_head.pairwise_predictor.0.weight"
    fix: Added experiments/remote_training/relational_gnn/best_model.pt to auto-detect paths
    file: src/mcp_ros2_bridge/server.py
    
  3_qwen_not_completing:
    problem: Qwen kept calling get_world_graph even when predicates > 0
    fix: Simplified prompt to be explicit about completion condition
    file: src/agents/qwen_agent.py

test_results:
  json_output_test: ✅ PASS
  mcp_e2e_test: ✅ PASS
  steps_to_complete: 2 (call get_world_graph → complete with 78 predicates)
  graph_info: 16 nodes, 78 spatial predicates, 42 interaction predicates

[SIMULATE_ACTION_INTEGRATION]
date: 2026-01-18
status: ✅ COMPLETE

checkpoints_available:
  full_training:
    path: experiments/remote_training/forward_dynamics_e2e/best_model.pt
    epochs: 100
    frames: 55k (full ALOHA dataset)
    gpu: RTX 3070 (8.2GB VRAM)
    delta_error: 0.0017 (1.7mm accuracy)
    confidence_range: 0.48-0.75
    training_time: 17 min pre-compute + 2.3 min training
    
  minimal_local:
    path: experiments/forward_dynamics/best_model.pt
    epochs: 5
    frames: 500
    gpu: RTX 500 (4GB)
    
implementation:
  1_server_auto_load:
    file: src/mcp_ros2_bridge/server.py
    change: Added auto-detection of ForwardDynamicsModel checkpoint
    priority: Full training checkpoint > local minimal checkpoint
    handles: External GNN encoder detection via state_dict key inspection
    
  2_test_script:
    file: scripts/test_simulate_action.py
    tests:
      - MCP client direct call: ✅ PASS
      - Qwen agent integration: ✅ PASS  
      - Custom action sequence: ✅ PASS

test_output_full_model:
  dataset_actions:
    recommendation: REPLAN
    min_confidence: 0.492
    steps: 3
    per_step_conf: [0.496, 0.495, 0.492]
    is_feasible: False (confidence < 0.7 threshold)
  custom_actions:
    recommendation: REPLAN
    min_confidence: 0.615
    steps: 2
    per_step_conf: [0.625, 0.615]
    delta_magnitude: 0.010-0.011
    is_feasible: False (confidence < 0.7 threshold)

feasibility_analysis:
  note: Model outputs is_feasible=False due to internal confidence_threshold=0.7
  breakdown:
    - feasibility_logit > 0: ✅ TRUE
    - confidence > 0.7: ❌ FALSE (model outputs ~0.5)
    - delta < 0.1: ✅ TRUE
  interpretation: Model is conservative by design. Lower threshold for production.

remote_training_machine:
  host: cip7g1.cip.cs.fau.de
  user: xi58pizy
  gpu: NVIDIA GeForce RTX 3070 (8.2GB VRAM)
  working_dir: /proj/ciptmp/xi58pizy/AI2MCP


[CURRENT_STATUS]
date: 2026-01-18
phase_9_fair_comparison: ✅ COMPLETE
phase_10_3_forward_dynamics: ✅ COMPLETE (full 55k training on RTX 3070)
phase_11_spatiotemporal_gnn: ✅ COMPLETE
phase_12_qwen_agent: ✅ COMPLETE (E2E validated)
figure_generation: ✅ COMPLETE (28 PNGs)
simulate_action_integration: ✅ COMPLETE (full model loaded, conf=0.49-0.62)
agent_benchmark: ✅ COMPLETE (Llama + Qwen 100% success)
  
completed_today:
  - QwenAgent implementation (src/agents/qwen_agent.py)
  - Qwen2.5:3b model pulled via Ollama
  - JSON parsing and action format validated
  - run_experiment.py updated with Qwen option
  - Swappable AI brain validated with two different LLMs
  - Parameterized all figure scripts with --format argument
  - Generated 28 PNG figures to figures/
  - Updated MasterPlan.md with completion status
  - Fixed observation extraction in base_agent.py
  - Fixed model checkpoint auto-detection in server.py
  - Fixed Qwen prompt to complete when predicates found
  - Integrated full ForwardDynamicsModel (55k frames) into MCP server
  - Server now auto-loads: experiments/remote_training/forward_dynamics_e2e/best_model.pt
  - Created simulate_action test script with threshold variations
  - Created agent benchmark script (Llama vs Qwen)
  - Validated simulate_action outputs meaningful confidence (0.49-0.62)

remaining:
  - Update thesis Results chapter
  - [Optional] Error recovery testing (malformed tool calls)
  - [Optional] Multi-step planning test (5+ tool calls)
  - [Optional] Evaluate SpatiotemporalGNN on temporal predicate stability

================================================================================
END CONTEXT DUMP
================================================================================

