# AI2MCP: A Standardized Middleware Architecture for Decoupled Robotic Intelligence

[![ROS 2](https://img.shields.io/badge/ROS%202-Humble-blue)](https://docs.ros.org/en/humble/)
[![Python](https://img.shields.io/badge/Python-3.10+-green)](https://python.org)
[![MCP](https://img.shields.io/badge/MCP-1.0-purple)](https://modelcontextprotocol.io/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red)](https://pytorch.org)
[![LeRobot](https://img.shields.io/badge/LeRobot-HuggingFace-yellow)](https://huggingface.co/lerobot)

> **Thesis Project**: Demonstrating that robotic intelligence can be treated as a swappable service using the Model Context Protocol (MCP).

## Key Results (Fair Comparison, 55k frames)

| Metric | RelationalGNN | MultiModalGNN | Winner |
|--------|---------------|---------------|--------|
| **Accuracy** | **97.03%** | 96.51% | âœ… Kinematic |
| **`is_near` F1** | **0.954** | 0.920 | âœ… Kinematic |
| **Latency** | **1.5ms** | 24ms | âœ… Kinematic (16Ã—) |
| **Model Size** | **0.81MB** | 2.14MB | âœ… Kinematic (2.6Ã—) |
| **Pass@1** | 88.2% | â€” | â€” |

### LLM Agent Benchmark

| Metric | Llama3.2 (3B) | Qwen2.5 (3B) | Winner |
|--------|---------------|--------------|--------|
| **Success Rate** | 100% | 100% | TIE |
| **Avg Steps** | 2.8 | **1.0** | âœ… Qwen |
| **Time-to-First-Action** | **425ms** | 1073ms | âœ… Llama |
| **Avg Total Time** | 2561ms | **1537ms** | âœ… Qwen (40% faster) |

> âš ï¸ **Finding**: RelationalGNN outperforms MultiModalGNN on ALL metrics. Vision integration (DINOv2) adds complexity without benefit on ALOHA â€” spatial predicates are solvable from joint positions alone.

## Overview

This project implements an **MCP-to-ROS 2 Bridge** that allows any AI model (Claude, Llama, GPT, etc.) to control robots through a standardized protocol. The key innovation is treating the AI "brain" as a swappable componentâ€”similar to how USB-C standardizes hardware connections.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CLOUD / WORKSTATION                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   LLM Agent   â”‚â—„â”€â”€â–ºâ”‚   MCP Client SDK  â”‚â—„â”€â”€â–ºâ”‚  GNN Reasoner   â”‚  â”‚
â”‚  â”‚ (Llama/Qwen)  â”‚    â”‚   (JSON-RPC/SSE)  â”‚    â”‚ (World Graph)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚ HTTP/SSE
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ROBOT / EDGE (ROS 2 Humble)                  â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚                     â”‚   MCP-ROS2 Bridge   â”‚                         â”‚
â”‚                     â”‚   (Tools/Resources) â”‚                         â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                 â”‚                                   â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚           â–¼                     â–¼                     â–¼             â”‚
â”‚       /cmd_vel              /scan                /camera            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Features

- **ðŸ”Œ Protocol-Driven**: Standardized MCP interface for AI-robot communication
- **ðŸ”„ Swappable AI**: Change the "brain" (Llama â†” Qwen) without modifying robot code
- **ðŸ§  Semantic Perception**: GNN-based world graph for structured environment understanding
- **ðŸ¤– LeRobot Integration**: Train on HuggingFace LeRobot datasets (ALOHA, PushT, etc.)
- **ðŸ“Š Explainability**: All AI decisions logged as tool calls and resource queries
- **â˜ï¸ Cloud-Edge Split**: Heavy compute on cloud, lightweight execution on robot
- **ðŸ“ˆ Predicate Prediction**: 9 spatial/interaction predicates with 97% accuracy
- **ðŸ”® Pre-Execution Simulation**: ForwardDynamicsModel verifies LLM plans before execution
- **â±ï¸ Temporal Stability**: SpatiotemporalGNN with GRU eliminates predicate flicker

## Quick Start

### Prerequisites

- Ubuntu 22.04
- ROS 2 Humble
- Python 3.10+
- (Optional) Gazebo Classic for simulation

### Installation

```bash
# Clone repository
git clone https://github.com/your-repo/AI2MCP.git
cd AI2MCP

# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install -e ".[dev]"

# Source ROS 2
source /opt/ros/humble/setup.bash
```

### Running the Demo

#### Option A: LeRobot GNN Pipeline (No ROS required)

```bash
source .venv/bin/activate

# Run with synthetic data (quick test)
python scripts/demo_lerobot_pipeline.py --synthetic --frames 100

# Run with real LeRobot dataset
python scripts/demo_lerobot_pipeline.py --repo lerobot/aloha_static_coffee --frames 200

# Train the GNN model
python scripts/train_relational_gnn.py --epochs 100 --dataset aloha
```

#### Option B: Full ROS 2 + Gazebo Demo

**Terminal 1: Start Gazebo Simulation**
```bash
source /opt/ros/humble/setup.bash
export TURTLEBOT3_MODEL=burger
ros2 launch turtlebot3_gazebo turtlebot3_world.launch.py
```

**Terminal 2: Start MCP-ROS2 Bridge**
```bash
source /opt/ros/humble/setup.bash
source .venv/bin/activate
./scripts/start_bridge.sh
```

**Terminal 3: Run AI Agent**
```bash
source .venv/bin/activate

# With Llama (requires Ollama running locally)
python scripts/run_experiment.py --agent llama --goal "Navigate to position (3, -3)"

# With Qwen (requires Ollama running locally)
python scripts/run_experiment.py --agent qwen --goal "Navigate to position (3, -3)"
```

#### Option C: LeRobot + Ollama Demo (No ROS required)

This is the simplest way to test the full LLM â†’ MCP â†’ GNN pipeline:

**Step 1: Install Ollama**
```bash
# Linux
curl -fsSL https://ollama.com/install.sh | sh

# Pull llama3.2 (3B, 2.0GB) and/or qwen2.5 (3B, 1.9GB)
ollama pull llama3.2
ollama pull qwen2.5:3b

# Verify
ollama list
```

**Step 2: Start MCP Server with LeRobot**
```bash
source .venv/bin/activate
python -m mcp_ros2_bridge.server --lerobot
```

**Step 3: Run LLM Agent (new terminal)**
```bash
source .venv/bin/activate

# Llama (slightly slower, uses more steps)
python scripts/run_experiment.py --agent llama --goal "Query world graph and report predicates"

# Qwen (40% faster, fewer steps) âœ… RECOMMENDED
python scripts/run_experiment.py --agent qwen --goal "Query world graph and report predicates"
```

**Expected Output:**
```json
{
  "success": true,
  "total_steps": 2,
  "duration_seconds": 1.5,
  "observation": {
    "world_graph": {"num_nodes": 16, "num_edges": 54},
    "spatial_predicates": 78,
    "interaction_predicates": 42
  }
}
```

## Project Structure

```
AI2MCP/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ mcp_ros2_bridge/      # MCP server with ROS 2 integration
â”‚   â”‚   â”œâ”€â”€ server.py         # SSE-based MCP server
â”‚   â”‚   â”œâ”€â”€ ros_node.py       # ROS 2 node wrapper
â”‚   â”‚   â”œâ”€â”€ tools/            # MCP tools (motion, perception, prediction)
â”‚   â”‚   â”‚   â”œâ”€â”€ motion.py     # Movement commands
â”‚   â”‚   â”‚   â”œâ”€â”€ perception.py # Sensor queries
â”‚   â”‚   â”‚   â””â”€â”€ prediction.py # GNN-based prediction tools
â”‚   â”‚   â””â”€â”€ resources/        # MCP resources (pose, scan, world_graph)
â”‚   â”‚       â”œâ”€â”€ lerobot_state.py  # LeRobot dataset resources
â”‚   â”‚       â””â”€â”€ world_graph.py    # Semantic graph resources
â”‚   â”‚
â”‚   â”œâ”€â”€ gnn_reasoner/         # Semantic scene understanding
â”‚   â”‚   â”œâ”€â”€ data_manager.py   # LeRobot dataset loading
â”‚   â”‚   â”œâ”€â”€ lerobot_transformer.py  # State â†’ Graph conversion
â”‚   â”‚   â”œâ”€â”€ benchmark.py      # Performance metrics (pass@k, latency)
â”‚   â”‚   â”œâ”€â”€ detector.py       # DETIC/GroundingDINO object detection
â”‚   â”‚   â”œâ”€â”€ depth.py          # ZoeDepth/MiDaS depth estimation
â”‚   â”‚   â”œâ”€â”€ camera.py         # Camera intrinsics & 3D projection
â”‚   â”‚   â”œâ”€â”€ graph_builder.py  # Sensor â†’ World Graph
â”‚   â”‚   â””â”€â”€ model/            # PyTorch Geometric GNN
â”‚   â”‚       â”œâ”€â”€ relational_gnn.py   # Kinematic GNN (97.03% acc) âœ… RECOMMENDED
â”‚   â”‚       â”œâ”€â”€ multimodal_gnn.py   # Vision+Kinematic GNN (96.51% acc)
â”‚   â”‚       â”œâ”€â”€ forward_dynamics.py # Pre-execution simulation (259K params)
â”‚   â”‚       â”œâ”€â”€ spatiotemporal_gnn.py # Temporal stability (~90% acc)
â”‚   â”‚       â””â”€â”€ scene_gnn.py        # Scene understanding
â”‚   â”‚
â”‚   â””â”€â”€ agents/               # Swappable AI agents
â”‚       â”œâ”€â”€ base_agent.py     # Abstract agent interface + MCPClient
â”‚       â”œâ”€â”€ llama_agent.py    # Llama3.2 via Ollama
â”‚       â””â”€â”€ qwen_agent.py     # Qwen2.5 via Ollama âœ… RECOMMENDED
â”‚
â”œâ”€â”€ experiments/              # Training & benchmark results
â”‚   â”œâ”€â”€ aloha_training/       # Local kinematic GNN (99.4% acc, 5k frames)
â”‚   â”œâ”€â”€ remote_training/      # Full 55k frame training (RTX 3070)
â”‚   â”‚   â”œâ”€â”€ relational_gnn/   # 97.03% acc âœ… BEST
â”‚   â”‚   â”œâ”€â”€ multimodal_gnn_55k_v2/  # 96.51% acc
â”‚   â”‚   â”œâ”€â”€ forward_dynamics_e2e/   # Î´=0.0017, conf=0.49-0.62
â”‚   â”‚   â””â”€â”€ spatiotemporal_gnn/     # ~90% acc (temporal)
â”‚   â”œâ”€â”€ comparison_final_real/  # Fair A vs C comparison
â”‚   â”œâ”€â”€ ablation_depth/       # Depth noise ablation
â”‚   â”œâ”€â”€ agent_benchmark.json  # Llama vs Qwen results
â”‚   â””â”€â”€ training/             # Synthetic baseline (95.9% acc)
â”‚
â”œâ”€â”€ figures/                  # Thesis figures (28 PNGs, auto-generated)
â”‚   â”œâ”€â”€ training_curves.png   # Loss/accuracy plots
â”‚   â”œâ”€â”€ architecture.png      # System diagram
â”‚   â”œâ”€â”€ comparison/           # A vs C comparison figures
â”‚   â”œâ”€â”€ forward_dynamics_*.png  # Phase 10 figures
â”‚   â””â”€â”€ stgnn_*.png           # Phase 11 figures
â”‚
â”œâ”€â”€ simulation/               # Gazebo simulation setup
â”‚   â”œâ”€â”€ launch/              # ROS 2 launch files
â”‚   â”œâ”€â”€ config/              # Nav2 parameters
â”‚   â””â”€â”€ worlds/              # Custom Gazebo worlds
â”‚
â”œâ”€â”€ scripts/                 # Utility scripts
â”‚   â”œâ”€â”€ train_relational_gnn.py   # Kinematic GNN training
â”‚   â”œâ”€â”€ train_multimodal_gnn.py   # MultiModal GNN training
â”‚   â”œâ”€â”€ train_forward_model.py    # ForwardDynamicsModel training
â”‚   â”œâ”€â”€ train_spatiotemporal_gnn.py # SpatiotemporalGNN training
â”‚   â”œâ”€â”€ compare_models.py         # A vs C benchmark
â”‚   â”œâ”€â”€ benchmark_agents.py       # Llama vs Qwen agent benchmark
â”‚   â”œâ”€â”€ demo_lerobot_pipeline.py  # Pipeline demo
â”‚   â”œâ”€â”€ run_experiment.py         # LLM agent runner
â”‚   â”œâ”€â”€ generate_thesis_figures.py
â”‚   â””â”€â”€ generate_comparison_figures.py
â”‚
â”œâ”€â”€ tests/                   # Test suite
â””â”€â”€ docs/                    # Documentation
```

## MCP Interface

### Tools (Actions)

#### Motion Tools
| Tool | Description |
|------|-------------|
| `move(linear_x, angular_z, duration_ms)` | Move with velocity for duration |
| `stop()` | Emergency stop |
| `rotate(angle_degrees)` | Rotate in place |
| `move_forward(distance_meters)` | Move forward by distance |

#### Perception Tools
| Tool | Description |
|------|-------------|
| `get_obstacle_distances(directions)` | Query obstacles in directions |
| `check_path_clear(distance, width)` | Check if path is navigable |
| `scan_surroundings(num_sectors)` | 360Â° obstacle scan |

#### Prediction Tools (LeRobot/GNN)
| Tool | Description |
|------|-------------|
| `get_world_graph(frame_idx)` | Get semantic graph with predicates |
| `predict_action_outcome(action, num_steps)` | Predict future state changes |
| `advance_frame()` | Move to next frame in trajectory |
| `set_frame(index)` | Jump to specific frame |
| `get_predicates(threshold)` | Get active spatial/interaction predicates |
| `simulate_action(action_sequence, confidence_threshold)` | Pre-execution verification |
| `project_future(action, horizon_steps)` | **NEW** Temporal predicate projection |

#### Pre-Execution Simulation (Phase 10) âœ…

The `simulate_action` tool enables LLM agents to **verify plans before physical execution**:

```python
# LLM proposes action sequence
result = await client.call_tool("simulate_action", {
    "action_sequence": [[0.1, 0.2, ...], [0.15, 0.25, ...]],  # 14-DoF actions
    "num_steps": 5,
    "confidence_threshold": 0.7
})

# Returns: {"recommendation": "EXECUTE" | "REPLAN", "confidence": 0.85, ...}
```

| Metric | Value |
|--------|-------|
| Model | ForwardDynamicsModel (259K params) |
| Training | 55k frames, 17 min pre-compute + 2.3 min training |
| Inference | 41ms |
| Delta Error | 0.0017 (1.7mm accuracy) |
| Confidence Range | 0.49â€“0.62 |

#### Temporal Stability (Phase 11) âœ…

The `project_future` tool uses SpatiotemporalGNN to **predict future predicates**:

```python
# AI asks: "If I move forward, what predicates will be active?"
result = await client.call_tool("project_future", {
    "action": [0.1, 0.0, ...],  # 14-DoF action
    "horizon_steps": 3
})

# Returns: predicted predicates at t+1, t+2, t+3 with confidence scores
```

| Metric | Value |
|--------|-------|
| Model | SpatiotemporalGNN (GRU + RelationalGNN) |
| Training | 55k frames, 47 min |
| Accuracy | ~90% |
| Sequence Length | 5 frames |

### Available LLM Agents

| Agent | Model | Backend | Status |
|-------|-------|---------|--------|
| **QwenAgent** | qwen2.5:3b (1.9GB) | Ollama | âœ… **RECOMMENDED** (40% faster) |
| LlamaAgent | llama3.2 (2.0GB) | Ollama | âœ… Validated |

Both agents achieve **100% success rate** on standardized goals. Qwen uses fewer steps and is faster overall.

### Resources (State)

#### Robot Resources
| Resource URI | Description |
|--------------|-------------|
| `robot://pose` | Current position (x, y, Î¸) |
| `robot://velocity` | Current velocity |
| `robot://scan/summary` | LiDAR scan summary by quadrant |
| `robot://scan/obstacles` | Detected obstacle clusters |
| `robot://world_graph` | Semantic scene graph |

#### LeRobot Resources
| Resource URI | Description |
|--------------|-------------|
| `lerobot://current_state` | Current observation state (14 joints) |
| `lerobot://world_graph` | GNN-processed relational graph |
| `lerobot://predicates` | Active predicates with confidence scores |
| `lerobot://dataset_info` | Dataset metadata (episodes, frames) |

### Predicate Types

The GNN predicts 9 binary predicates:

**Spatial (5):** `is_near`, `is_above`, `is_below`, `is_left_of`, `is_right_of`

**Interaction (4):** `is_holding`, `is_contacting`, `is_approaching`, `is_retracting`

## Thesis Validation

The key experiment demonstrates **swappable intelligence**:

1. Run task with Llama3.2 agent
2. Run identical task with Qwen2.5 agent  
3. **No robot-side code changes** between runs

```bash
# Run agent benchmark (10 goals, both agents)
python scripts/benchmark_agents.py

# Or run single agent
python scripts/run_experiment.py --agent llama --goal "Get world graph"
python scripts/run_experiment.py --agent qwen --goal "Get world graph"
```

Results are saved to `experiments/agent_benchmark.json`.

## Experiment Results

### GNN Training Performance

| Model | Dataset | Frames | Accuracy | Training Time | GPU |
|-------|---------|--------|----------|---------------|-----|
| Synthetic Baseline | Synthetic | 1k | 95.9% | 21s | RTX 500 |
| RelationalGNN (local) | ALOHA | 5k | 99.4% | 205s | RTX 500 |
| **RelationalGNN** | ALOHA | **55k** | **97.03%** | 29 min | RTX 3070 |
| MultiModalGNN | ALOHA | 55k | 96.51% | 31 min | RTX 3070 |
| ForwardDynamicsModel | ALOHA | 55k | Î´=0.0017 | 2.3 min | RTX 3070 |
| **SpatiotemporalGNN** | ALOHA | 55k | **~90%** | 47 min | RTX 3070 |

> **Note**: The 5k-frame local training shows higher accuracy (99.4%) than 55k remote (97.03%) due to overfitting on the smaller dataset. The 55k results are more representative.

### Inference Benchmark (200 frames, trained model)

| Metric | Value |
|--------|-------|
| Predicate Accuracy | 92.5% |
| Precision | 84.5% |
| Recall | 79.9% |
| F1 Score | 82.1% |
| Pass@1 | 88.2% |
| Pass@3 | 98.2% |
| Inference Latency (mean) | 12.6ms |
| Inference Latency (p95) | 12.7ms |
| Graph Construction | 1.7ms |
| Serialization | 0.15ms |
| Protocol Overhead | 30.4% |

### Predicate Distribution (200 frames)

| Predicate | Detections |
|-----------|------------|
| `is_near` | 8,539 |
| `is_right_of` | 4,769 |
| `is_left_of` | 4,727 |
| `is_above` | 0 |
| `is_below` | 0 |

*Interaction predicates (`is_holding`, `is_approaching`, etc.) require intentional manipulation behavior not present in synthetic test data.*

### Reproducibility

```bash
# Reproduce training
python scripts/train_relational_gnn.py --epochs 100 --dataset aloha

# Reproduce benchmark
python scripts/demo_lerobot_pipeline.py --synthetic --frames 200 \
    --model experiments/aloha_training/best_model.pt \
    --output experiments/benchmark_reproduced.json

# Generate thesis figures
python scripts/generate_thesis_figures.py --output figures/
```

## Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `MCP_SERVER_HOST` | `0.0.0.0` | MCP server bind address |
| `MCP_SERVER_PORT` | `8080` | MCP server port |
| `OLLAMA_URL` | `http://localhost:11434` | Ollama endpoint |
| `OLLAMA_MODEL` | `qwen2.5:3b` | Ollama model name (qwen2.5:3b or llama3.2) |

### Agent Configuration

```python
from agents.base_agent import AgentConfig
from agents.qwen_agent import QwenAgent  # or LlamaAgent

config = AgentConfig(
    mcp_server_url="http://localhost:8080",
    max_steps=10,
    timeout_seconds=30.0,
)

agent = QwenAgent(config=config, model="qwen2.5:3b")  # âœ… RECOMMENDED
# or: agent = LlamaAgent(config=config, model="llama3.2")
```

## Development

```bash
# Run tests
pytest tests/ -v

# Type checking
mypy src/

# Linting
ruff check src/

# Format
ruff format src/
```

## Known Issues & Limitations

| Issue | Status | Workaround |
|-------|--------|------------|
| MCP SSE resource transport bug | âš ï¸ Library bug | Agent uses tool results instead of resources |
| LLM sends string numbers (`"0"` vs `0`) | âœ… Fixed | Auto-coerced in `MCPClient.call_tool()` |
| Llama 3B loops on complex prompts | âœ… Fixed | Simplified system prompt with explicit rules |
| ZoeDepth installation (timm version) | âš ï¸ | Falls back to MiDaS (relative depth only) |
| `is_holding`/`is_contacting` = 0.000 F1 | âš ï¸ Data limitation | ALOHA lacks contact annotations |

### Contact Predicates Limitation

`is_holding` and `is_contacting` show 0.000 F1 on real ALOHA data because:
- Only ~0.7% positive `is_holding` edges in dataset
- No explicit contact annotations available
- Heuristic labels (gripper near object + closed) are insufficient

**Solution**: Requires annotated dataset with explicit contact labels, F/T sensing, or tactile integration.

> **Note on Resource Bug**: MCP SDK v1.25.0 has a bug in the SSE transport layer for resources. Tools work correctly. The agent uses `get_world_graph` tool calls instead of resource reads.

## Vision Integration

This project includes two approaches for integrating visual object detection with the kinematic GNN:

### Option A: RelationalGNN (Kinematic + Geometric Fusion) âœ… RECOMMENDED
```
JointState â†’ Graph â†’ RelationalGNN â†’ Predicates
(Optional) Image â†’ GroundingDINO â†’ Depth â†’ 3D Objects â†’ Graph
```
- **Latency:** 1.5ms (GNN only) / 297ms (with real vision)
- **Accuracy:** 97.03%
- **Best for:** All use cases on ALOHA-style datasets

### Option C: MultiModalGNN (DINOv2 Cross-Attention)
```
Image â†’ DINOv2 â†’ RoI Pool â†’ Cross-Attention â†’ MultiModalGNN
```
- **Latency:** 24ms
- **Accuracy:** 96.51%
- **Best for:** Datasets where objects are NOT encoded in kinematics

### Fair Comparison Results (55k vs 55k frames)

| Metric | Option A | Option C | Winner |
|--------|----------|----------|--------|
| Micro Accuracy | **97.03%** | 96.51% | **A (+0.5%)** |
| Macro F1 | **0.358** | 0.348 | **A (+2.9%)** |
| `is_near` F1 | **0.954** | 0.920 | **A** |
| `is_approaching` F1 | **0.182** | 0.156 | **A** |
| Latency | **1.5ms** | 24ms | **A (16Ã— faster)** |
| Peak Memory | **19.4MB** | 141.8MB | **A (7Ã— less)** |
| Model Size | **0.81MB** | 2.14MB | **A (2.6Ã— smaller)** |

### Honest E2E Latency (Real Vision on RTX 3070)

| Component | Time |
|-----------|------|
| GroundingDINO detection | 217-234ms |
| Depth Anything V2 | 61ms |
| GNN inference | 1.4ms |
| **Total E2E** | **297-332ms** |

> âš ï¸ **Note**: The 2.4ms latency previously reported was with **mock detectors**. Real vision adds ~300ms.

```bash
# Train MultiModalGNN
python scripts/train_multimodal_gnn.py --epochs 100 --output experiments/multimodal

# Compare models
python scripts/compare_models.py --model-a experiments/aloha_training/best_model.pt \
    --model-c experiments/multimodal_aloha/best_model.pt --frames 500

# Run ablation study
python scripts/ablation_depth_noise.py --frames 200 --output experiments/ablation_depth
```

### Hardware Requirements

| Mode | GPU | VRAM | Stack |
|------|-----|------|-------|
| **Development** | Any (CPU OK) | - | Mock detectors |
| **Kinematic GNN** | RTX 500+ | 1GB | RelationalGNN only |
| **MultiModal (Heavy)** | RTX 4080+ | 4GB+ | GroundingDINO + ZoeDepth + DINOv2 |
| **MultiModal (Edge)** | RTX 500 | 1GB | YOLO-World + DepthAnything V2 Small |

> **Note**: The "heavy" vision stack (GroundingDINO + ZoeDepth + DINOv2) requires ~3.8GB VRAM. For RTX 500 (4GB) deployment, use mock detectors during development or the edge-native stack (YOLO-World + Depth Anything V2 Small, ~0.9GB).

---

## Research Contributions

1. **NÃ—M â†’ N+M Complexity**: Single MCP interface per robot connects to any model
2. **Explainable Robot AI**: All decisions logged as structured tool calls
3. **Semantic Perception**: GNN-processed world graphs with 97% predicate accuracy
4. **Protocol-Driven Robotics**: Foundation for multi-robot, multi-agent systems
5. **LeRobot Integration**: First MCP bridge for HuggingFace robotics datasets
6. **Swappable AI Validated**: Llama3.2 + Qwen2.5 â†’ MCP â†’ GNN E2E (100% success rate)
7. **Pre-Execution Simulation**: ForwardDynamicsModel for LLM plan verification before physical execution
8. **Fair Architecture Comparison**: RelationalGNN vs MultiModalGNN on 55k frames â€” kinematic wins
9. **Temporal Predicate Stability**: SpatiotemporalGNN with GRU eliminates frame-to-frame flicker (~90% acc)
10. **Agent Benchmark**: Qwen 40% faster than Llama, 65% fewer steps to complete tasks

## Citation

```bibtex
@mastersthesis{sazzad2025mcp,
  title={A Standardized Middleware Architecture for Decoupled Robotic 
         Intelligence using the Model Context Protocol},
  author={Sazzad, Khaled},
  year={2025},
  school={Your University}
}
```

## License

MIT License - See [LICENSE](LICENSE) for details.

## Acknowledgments

- [Model Context Protocol](https://modelcontextprotocol.io/) by Anthropic
- [ROS 2](https://ros.org/) by Open Robotics
- [PyTorch Geometric](https://pyg.org/) for GNN implementation
- [LeRobot](https://huggingface.co/lerobot) by HuggingFace for robotics datasets
- [ALOHA](https://tonyzhaozh.github.io/aloha/) for bimanual manipulation data

