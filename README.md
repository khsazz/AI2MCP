# AI2MCP: A Standardized Middleware Architecture for Decoupled Robotic Intelligence

[![ROS 2](https://img.shields.io/badge/ROS%202-Humble-blue)](https://docs.ros.org/en/humble/)
[![Python](https://img.shields.io/badge/Python-3.10+-green)](https://python.org)
[![MCP](https://img.shields.io/badge/MCP-1.0-purple)](https://modelcontextprotocol.io/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red)](https://pytorch.org)
[![LeRobot](https://img.shields.io/badge/LeRobot-HuggingFace-yellow)](https://huggingface.co/lerobot)

> **Thesis Project**: Demonstrating that robotic intelligence can be treated as a swappable service using the Model Context Protocol (MCP).

## Key Results

| Metric | Value |
|--------|-------|
| **Training Accuracy** | 99.4% (ALOHA dataset) |
| **Pass@1 Prediction** | 88.2% |
| **Pass@3 Prediction** | 98.2% |
| **F1 Score** | 82.1% |
| **Inference Latency** | 12.6ms (p95) |
| **Protocol Overhead** | 30.4% |

## Overview

This project implements an **MCP-to-ROS 2 Bridge** that allows any AI model (Claude, Llama, GPT, etc.) to control robots through a standardized protocol. The key innovation is treating the AI "brain" as a swappable componentâ€”similar to how USB-C standardizes hardware connections.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CLOUD / WORKSTATION                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   LLM Agent   â”‚â—„â”€â”€â–ºâ”‚   MCP Client SDK  â”‚â—„â”€â”€â–ºâ”‚  GNN Reasoner   â”‚  â”‚
â”‚  â”‚ (Claude/Llama)â”‚    â”‚   (JSON-RPC/SSE)  â”‚    â”‚ (World Graph)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚ HTTP/SSE
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ROBOT / EDGE (ROS 2 Humble)                  â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚                     â”‚   MCP-ROS2 Bridge   â”‚                         â”‚
â”‚                     â”‚   (Tools/Resources) â”‚                         â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                 â”‚                                   â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚           â–¼                     â–¼                     â–¼             â”‚
â”‚       /cmd_vel              /scan                /camera            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Features

- **ğŸ”Œ Protocol-Driven**: Standardized MCP interface for AI-robot communication
- **ğŸ”„ Swappable AI**: Change the "brain" (Claude â†” Llama) without modifying robot code
- **ğŸ§  Semantic Perception**: GNN-based world graph for structured environment understanding
- **ğŸ¤– LeRobot Integration**: Train on HuggingFace LeRobot datasets (ALOHA, PushT, etc.)
- **ğŸ“Š Explainability**: All AI decisions logged as tool calls and resource queries
- **â˜ï¸ Cloud-Edge Split**: Heavy compute on cloud, lightweight execution on robot
- **ğŸ“ˆ Predicate Prediction**: 9 spatial/interaction predicates with 99.4% accuracy

## Quick Start

### Prerequisites

- Ubuntu 22.04
- ROS 2 Humble
- Python 3.10+
- (Optional) Gazebo Classic for simulation

### Installation

```bash
# Clone repository
git clone https://github.com/your-repo/AI2MCP.git
cd AI2MCP

# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install -e ".[dev]"

# Source ROS 2
source /opt/ros/humble/setup.bash
```

### Running the Demo

#### Option A: LeRobot GNN Pipeline (No ROS required)

```bash
source .venv/bin/activate

# Run with synthetic data (quick test)
python scripts/demo_lerobot_pipeline.py --synthetic --frames 100

# Run with real LeRobot dataset
python scripts/demo_lerobot_pipeline.py --repo lerobot/aloha_static_coffee --frames 200

# Train the GNN model
python scripts/train_relational_gnn.py --epochs 100 --dataset aloha
```

#### Option B: Full ROS 2 + Gazebo Demo

**Terminal 1: Start Gazebo Simulation**
```bash
source /opt/ros/humble/setup.bash
export TURTLEBOT3_MODEL=burger
ros2 launch turtlebot3_gazebo turtlebot3_world.launch.py
```

**Terminal 2: Start MCP-ROS2 Bridge**
```bash
source /opt/ros/humble/setup.bash
source .venv/bin/activate
./scripts/start_bridge.sh
```

**Terminal 3: Run AI Agent**
```bash
source .venv/bin/activate

# With Claude (requires ANTHROPIC_API_KEY)
export ANTHROPIC_API_KEY="your-key"
python scripts/run_experiment.py --agent claude --goal "Navigate to position (3, -3)"

# With Llama (requires Ollama running locally)
python scripts/run_experiment.py --agent llama --goal "Navigate to position (3, -3)"
```

## Project Structure

```
AI2MCP/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ mcp_ros2_bridge/      # MCP server with ROS 2 integration
â”‚   â”‚   â”œâ”€â”€ server.py         # SSE-based MCP server
â”‚   â”‚   â”œâ”€â”€ ros_node.py       # ROS 2 node wrapper
â”‚   â”‚   â”œâ”€â”€ tools/            # MCP tools (motion, perception, prediction)
â”‚   â”‚   â”‚   â”œâ”€â”€ motion.py     # Movement commands
â”‚   â”‚   â”‚   â”œâ”€â”€ perception.py # Sensor queries
â”‚   â”‚   â”‚   â””â”€â”€ prediction.py # GNN-based prediction tools
â”‚   â”‚   â””â”€â”€ resources/        # MCP resources (pose, scan, world_graph)
â”‚   â”‚       â”œâ”€â”€ lerobot_state.py  # LeRobot dataset resources
â”‚   â”‚       â””â”€â”€ world_graph.py    # Semantic graph resources
â”‚   â”‚
â”‚   â”œâ”€â”€ gnn_reasoner/         # Semantic scene understanding
â”‚   â”‚   â”œâ”€â”€ data_manager.py   # LeRobot dataset loading
â”‚   â”‚   â”œâ”€â”€ lerobot_transformer.py  # State â†’ Graph conversion
â”‚   â”‚   â”œâ”€â”€ benchmark.py      # Performance metrics (pass@k, latency)
â”‚   â”‚   â”œâ”€â”€ model/            # PyTorch Geometric GNN
â”‚   â”‚   â”‚   â”œâ”€â”€ relational_gnn.py  # Predicate prediction (99.4% acc)
â”‚   â”‚   â”‚   â””â”€â”€ scene_gnn.py       # Scene understanding
â”‚   â”‚   â”œâ”€â”€ detector.py       # YOLO-based object detection
â”‚   â”‚   â””â”€â”€ graph_builder.py  # Sensor â†’ World Graph
â”‚   â”‚
â”‚   â””â”€â”€ agents/               # Swappable AI agents
â”‚       â”œâ”€â”€ base_agent.py     # Abstract agent interface
â”‚       â”œâ”€â”€ claude_agent.py   # Anthropic Claude implementation
â”‚       â””â”€â”€ llama_agent.py    # Local Llama (Ollama/vLLM)
â”‚
â”œâ”€â”€ experiments/              # Training & benchmark results
â”‚   â”œâ”€â”€ aloha_training/       # ALOHA dataset training (99.4% acc)
â”‚   â”‚   â”œâ”€â”€ best_model.pt     # Best checkpoint
â”‚   â”‚   â””â”€â”€ training_history.json
â”‚   â”œâ”€â”€ training/             # Synthetic baseline (95.9% acc)
â”‚   â””â”€â”€ benchmark_with_trained_model.json
â”‚
â”œâ”€â”€ figures/                  # Thesis figures (auto-generated)
â”‚   â”œâ”€â”€ training_curves.png   # Loss/accuracy plots
â”‚   â”œâ”€â”€ pass_at_k.png         # Prediction accuracy
â”‚   â”œâ”€â”€ classification_metrics.png
â”‚   â””â”€â”€ architecture.png      # System diagram
â”‚
â”œâ”€â”€ simulation/               # Gazebo simulation setup
â”‚   â”œâ”€â”€ launch/              # ROS 2 launch files
â”‚   â”œâ”€â”€ config/              # Nav2 parameters
â”‚   â””â”€â”€ worlds/              # Custom Gazebo worlds
â”‚
â”œâ”€â”€ scripts/                 # Utility scripts
â”‚   â”œâ”€â”€ train_relational_gnn.py   # GNN training
â”‚   â”œâ”€â”€ demo_lerobot_pipeline.py  # Pipeline demo
â”‚   â””â”€â”€ generate_thesis_figures.py
â”‚
â”œâ”€â”€ tests/                   # Test suite
â””â”€â”€ docs/                    # Documentation
```

## MCP Interface

### Tools (Actions)

#### Motion Tools
| Tool | Description |
|------|-------------|
| `move(linear_x, angular_z, duration_ms)` | Move with velocity for duration |
| `stop()` | Emergency stop |
| `rotate(angle_degrees)` | Rotate in place |
| `move_forward(distance_meters)` | Move forward by distance |

#### Perception Tools
| Tool | Description |
|------|-------------|
| `get_obstacle_distances(directions)` | Query obstacles in directions |
| `check_path_clear(distance, width)` | Check if path is navigable |
| `scan_surroundings(num_sectors)` | 360Â° obstacle scan |

#### Prediction Tools (LeRobot/GNN)
| Tool | Description |
|------|-------------|
| `get_world_graph(frame_idx)` | Get semantic graph with predicates |
| `predict_action_outcome(action, num_steps)` | Predict future state changes |
| `advance_frame()` | Move to next frame in trajectory |
| `set_frame(index)` | Jump to specific frame |
| `get_predicates(threshold)` | Get active spatial/interaction predicates |

### Resources (State)

#### Robot Resources
| Resource URI | Description |
|--------------|-------------|
| `robot://pose` | Current position (x, y, Î¸) |
| `robot://velocity` | Current velocity |
| `robot://scan/summary` | LiDAR scan summary by quadrant |
| `robot://scan/obstacles` | Detected obstacle clusters |
| `robot://world_graph` | Semantic scene graph |

#### LeRobot Resources
| Resource URI | Description |
|--------------|-------------|
| `lerobot://current_state` | Current observation state (14 joints) |
| `lerobot://world_graph` | GNN-processed relational graph |
| `lerobot://predicates` | Active predicates with confidence scores |
| `lerobot://dataset_info` | Dataset metadata (episodes, frames) |

### Predicate Types

The GNN predicts 9 binary predicates:

**Spatial (5):** `is_near`, `is_above`, `is_below`, `is_left_of`, `is_right_of`

**Interaction (4):** `is_holding`, `is_contacting`, `is_approaching`, `is_retracting`

## Thesis Validation

The key experiment demonstrates **swappable intelligence**:

1. Run navigation task with Claude agent
2. Run identical task with Llama agent  
3. **No robot-side code changes** between runs

```bash
# Run comparison experiment
python scripts/run_experiment.py --agent both --goal "Explore the room"
```

Results are saved to `experiments/` for thesis analysis.

## Experiment Results

### GNN Training Performance

| Dataset | Epochs | Final Accuracy | Best Val Loss | Training Time |
|---------|--------|----------------|---------------|---------------|
| Synthetic | 50 | 95.9% | 0.1086 | 21s |
| **ALOHA** | 100 | **99.4%** | **0.0232** | 205s |

### Inference Benchmark (200 frames, trained model)

| Metric | Value |
|--------|-------|
| Predicate Accuracy | 92.5% |
| Precision | 84.5% |
| Recall | 79.9% |
| F1 Score | 82.1% |
| Pass@1 | 88.2% |
| Pass@3 | 98.2% |
| Inference Latency (mean) | 12.6ms |
| Inference Latency (p95) | 12.7ms |
| Graph Construction | 1.7ms |
| Serialization | 0.15ms |
| Protocol Overhead | 30.4% |

### Predicate Distribution (200 frames)

| Predicate | Detections |
|-----------|------------|
| `is_near` | 8,539 |
| `is_right_of` | 4,769 |
| `is_left_of` | 4,727 |
| `is_above` | 0 |
| `is_below` | 0 |

*Interaction predicates (`is_holding`, `is_approaching`, etc.) require intentional manipulation behavior not present in synthetic test data.*

### Reproducibility

```bash
# Reproduce training
python scripts/train_relational_gnn.py --epochs 100 --dataset aloha

# Reproduce benchmark
python scripts/demo_lerobot_pipeline.py --synthetic --frames 200 \
    --model experiments/aloha_training/best_model.pt \
    --output experiments/benchmark_reproduced.json

# Generate thesis figures
python scripts/generate_thesis_figures.py --output figures/
```

## Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `MCP_SERVER_HOST` | `0.0.0.0` | MCP server bind address |
| `MCP_SERVER_PORT` | `8080` | MCP server port |
| `ANTHROPIC_API_KEY` | - | Claude API key |
| `OLLAMA_URL` | `http://localhost:11434` | Ollama endpoint |

### Agent Configuration

```python
from agents.base_agent import AgentConfig
from agents.claude_agent import ClaudeAgent

config = AgentConfig(
    mcp_server_url="http://localhost:8080",
    max_steps=50,
    timeout_seconds=30.0,
)

agent = ClaudeAgent(config=config, model="claude-sonnet-4-20250514")
```

## Development

```bash
# Run tests
pytest tests/ -v

# Type checking
mypy src/

# Linting
ruff check src/

# Format
ruff format src/
```

## Research Contributions

1. **NÃ—M â†’ N+M Complexity**: Single MCP interface per robot connects to any model
2. **Explainable Robot AI**: All decisions logged as structured tool calls
3. **Semantic Perception**: GNN-processed world graphs with 99.4% predicate accuracy
4. **Protocol-Driven Robotics**: Foundation for multi-robot, multi-agent systems
5. **LeRobot Integration**: First MCP bridge for HuggingFace robotics datasets
6. **Real-time Inference**: 12.6ms latency enables 50+ Hz control loops

## Citation

```bibtex
@mastersthesis{sazzad2025mcp,
  title={A Standardized Middleware Architecture for Decoupled Robotic 
         Intelligence using the Model Context Protocol},
  author={Sazzad, Khaled},
  year={2025},
  school={Your University}
}
```

## License

MIT License - See [LICENSE](LICENSE) for details.

## Acknowledgments

- [Model Context Protocol](https://modelcontextprotocol.io/) by Anthropic
- [ROS 2](https://ros.org/) by Open Robotics
- [PyTorch Geometric](https://pyg.org/) for GNN implementation
- [LeRobot](https://huggingface.co/lerobot) by HuggingFace for robotics datasets
- [ALOHA](https://tonyzhaozh.github.io/aloha/) for bimanual manipulation data

